<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ai on Arguable Intelligence</title>
    <link>https://ojmason.github.io/categories/ai/index.xml</link>
    <description>Recent content in Ai on Arguable Intelligence</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>(C) Oliver Mason</copyright>
    <atom:link href="https://ojmason.github.io/categories/ai/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>History Repeats Itself</title>
      <link>https://ojmason.github.io/blog/history-repeats-itself/</link>
      <pubDate>Fri, 07 Apr 2017 11:11:30 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/history-repeats-itself/</guid>
      <description>&lt;p&gt;At the AAAI-84 conference, more than 30 years ago, was a panel discussion on &lt;em&gt;The Dark Ages of AI&lt;/em&gt;. Drew McDermott
said in the article in AI Magazine (vol 6 no 3 1985):&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In spite of all the commercial hustle and bustle around AI these days there&amp;rsquo;s a mood [&amp;hellip;] of deep unease among
AI reserachers [&amp;hellip;]. This unease is due to the worry that that perhaps expectations about AI are too high,
and that this will eventually end in disaster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have a real feeling of deja-vu. We have been there before, in the late 1960s, when the failed promises of
instant high-quality machine translation caused what was dubbed the &lt;em&gt;AI Winter&lt;/em&gt;, with all research funding
being stopped. AI recovered, and twenty years later the same happened. And now again.&lt;/p&gt;

&lt;p&gt;The situation has somewhat changed, as we have an abundance of data and processing power, but still no intelligence.
The algorithms are still the same old ones we had before, only on a larger scale.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34;&gt;Artificial neural networks&lt;/a&gt; are faster,
and with &lt;a href=&#34;https://en.wikipedia.org/wiki/Deep_learning&#34;&gt;Deep Learning&lt;/a&gt; they have become easier to use. We are replacing human engineering work (feature selection etc)
by clever algorithms and are letting the machine find things out by itself.&lt;/p&gt;

&lt;p&gt;The problem is that it works fine in some situations, and then it suddenly becomes the magic bullet. Everybody now has
to do Deep Learning, as it has become the new marketing buzzword. AI is everywhere, but also nowhere, really. We do
not know anything more about the world than we did before, but we can get fast computers to recognise patterns in
data. &lt;a href=&#34;https://ojmason.github.io/blog/arguable-intelligence/&#34;&gt;No intelligence there&lt;/a&gt;. And if you are trying to calm down expectations you are not
&amp;lsquo;disruptive&amp;rsquo; enough.&lt;/p&gt;

&lt;p&gt;It is only a question of time until it all comes crashing down, and AI will be be blamed for empty promises. We&amp;rsquo;ve
been there before.&lt;/p&gt;

&lt;p&gt;This all fits into the overall pattern of human behaviour. In Europe we are forgetting the lessons from the early
20th century, and nationalism is on the rise; it might seem a bit over the top to compare the current situation to
1930s Germany, but the point is that at the time people didn&amp;rsquo;t know what was happening. We now have the benefit of
knowing where it can lead to, and it would be foolish to ignore the warning signs.&lt;/p&gt;

&lt;p&gt;In AI as in society: history always repeats itself, and if you forget the past, you are condemmed to repeat it.
(&lt;a href=&#34;https://en.wikipedia.org/wiki/George_Santayana&#34;&gt;George Santayana&lt;/a&gt;)&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Note: thanks to @DiegoKuonen for tweeting the reference to the AAAI-84 paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Arguable Intelligence</title>
      <link>https://ojmason.github.io/blog/arguable-intelligence/</link>
      <pubDate>Tue, 28 Mar 2017 16:45:58 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/arguable-intelligence/</guid>
      <description>&lt;p&gt;I work in the field of AI. I studied it at university, I read books about AI in my spare time,
implement toy systems that operate in the field, and I try to keep uptodate with current developments.
And it infuriates me&amp;hellip;&lt;/p&gt;

&lt;p&gt;Artificial Intelligence has moved from an academic/applied subject to a marketing buzzword. Every
washing machine is now &amp;ldquo;powered by AI&amp;rdquo;. So AI means everything and nothing these days. Let&amp;rsquo;s look at
the traditional definitions:&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;Hard AI&lt;/dt&gt;
&lt;dd&gt;the cognitive processes of human beings are implemented in software. By doing so, a computer
basically functions like a human being, the brain being equivalent to a micro processor.&lt;/dd&gt;
&lt;dt&gt;Soft AI&lt;/dt&gt;
&lt;dd&gt;the cognitive system of a human being is imitated by computer software. There is no claim that
this models human cognition, but the outcomes are the same.&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;This dichotomy was current when I learned about AI. But now there is a new element which arose in the
past couple of decades with the availability of large amounts of data in all sorts of domains: machine
learning (ML). In ML you basically train a mathematical model with data, which allows you to classify
other data items (which your system has not seen before). Or maybe map between one item (eg an English phrase)
and another (eg an equivalent German phrase). The difference here is that there is &lt;em&gt;no human cognition involved&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;AI is a direct reference to cognition, hence the &amp;lsquo;I&amp;rsquo; of it, but ML is not. It&amp;rsquo;s an automaton, which has no
understanding of what it is doing. In early AI systems, domain knowledge was usually modelled in a
representation system, a logical language, a semantic network, whatever.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning&#34;&gt;Knowledge representation&lt;/a&gt; was/is an
important aspect of AI, as you cannot have AI without knowledge. In an ML system, knowledge is encoded in
model parameters, numerical values that have no meaning outside the model.&lt;/p&gt;

&lt;p&gt;One point that motivates me to work in AI is that we learn about how humans process the world. We find out what
is important, how me make decision, how we evaluate what is better or worse. In ML we just feed a black box with
examples that we have marked up as good or bad, and we get a black box that can distinguish between these categories
with a more or less acceptable accuracy. What have we learned about the process of making that distinction?&lt;/p&gt;

&lt;p&gt;Nothing.&lt;/p&gt;

&lt;p&gt;To re-iterate, machine learning is not artificial intelligence. It is statistical engineering. I&amp;rsquo;m not denying
that it is useful, but calling it AI is plain wrong, and will undoubtedly lead to a new
&lt;a href=&#34;https://en.wikipedia.org/wiki/AI_winter&#34;&gt;AI winter&lt;/a&gt; once the bubble bursts.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>