<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Arguable Intelligence</title>
    <link>https://ojmason.github.io/blog/index.xml</link>
    <description>Recent content in Blogs on Arguable Intelligence</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <copyright>(C) Oliver Mason</copyright>
    <lastBuildDate>Mon, 10 Apr 2017 11:27:24 +0100</lastBuildDate>
    <atom:link href="https://ojmason.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Toki Pona</title>
      <link>https://ojmason.github.io/blog/toki-pona/</link>
      <pubDate>Mon, 10 Apr 2017 11:27:24 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/toki-pona/</guid>
      <description>&lt;p&gt;Since working in computational linguistics I have been interested in constructed languages (&amp;lsquo;conlangs&amp;rsquo;).
When trying to process any natural language with computer programs, you constantly run into inconvenient
exceptions, in morphology, syntax, etc. A conlang such as &lt;a href=&#34;https://en.wikipedia.org/wiki/Esperanto&#34;&gt;Esperanto&lt;/a&gt;
promises a great simplification: as it is completely regular, NLP software is likely to be much simpler and
less complex. However, given that Esperanto is a full-scale language, it&amp;rsquo;s still not trivial to work with. It&amp;rsquo;s
got a large vocabulary, and the syntax is not that easy to parse.&lt;/p&gt;

&lt;p&gt;In a post on the &lt;a href=&#34;http://esperanto.stackexchange.com/&#34;&gt;Esperanto Language Stack Exchange&lt;/a&gt; I then heard of another
conlang, &lt;a href=&#34;https://en.wikipedia.org/wiki/Toki_Pona&#34;&gt;Toki Pona&lt;/a&gt;. This is billed as a &lt;em&gt;minimalist&lt;/em&gt; language: it only
has about 120 words, and a very fixed and simplistic sentence structure. Unlike Esperanto, it is not really a
proper language for everyday use, but more of a philosophical experiment. How does your language influence the
way you think about the world? This is kind of related to the &lt;a href=&#34;https://en.wikipedia.org/wiki/Linguistic_relativity&#34;&gt;Sapir-Whorf Hypothesis&lt;/a&gt;.
When you have to limit yourself to describing/paraphrasing everything with a limited vocabulary, you need to
reflect more about what it is you&amp;rsquo;re talking about. For example, a friend is a &lt;em&gt;jan pona&lt;/em&gt; (&amp;lsquo;good person&amp;rsquo;), and a
bad person is a &lt;em&gt;jan ike&lt;/em&gt;. So how do you say &amp;lsquo;bad friend&amp;rsquo;? You are not able to say someone is both good and bad
at the same time. So a friend cannot be a bad person, or vice versa.&lt;/p&gt;

&lt;p&gt;Toki Pona arguably is a toy language only. There is just one word for &lt;em&gt;fruit&lt;/em&gt;, &lt;em&gt;vegetable&lt;/em&gt;, etc.: &lt;em&gt;kili&lt;/em&gt;. If you want
to say &lt;em&gt;banana&lt;/em&gt;, you say &lt;em&gt;kili jelo&lt;/em&gt; (&amp;ldquo;yellow fruit&amp;rdquo;). If you want to talk about lemons as well, you&amp;rsquo;re out of luck.
It&amp;rsquo;s very context dependent, and definitely not useful for a scientific treatise, or even recipes. But it&amp;rsquo;s fine
for basic stories, myths and legends, and so on. And, most importantly, it&amp;rsquo;s easy to learn. No morphology. Very
limited syntax. Small vocabulary. The main difficulty is to express yourself given those limited means. But
we grow only when challenged!&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m interested in doing NLP with Toki Pona, as it is so limited. It should be possible to quickly get to the semantic
or pragmatic levels, as morphology and syntax will be dealt with easily. Analysing and generating sentences should
be extremely easy. More on that as it materialises.&lt;/p&gt;

&lt;p&gt;If you&amp;rsquo;re interested in languages, and what to explore the way you express meaning, give Toki Pona a try. There
are various on-line resources available, plus a text book &lt;a href=&#34;http://amzn.to/2oN4Zo7&#34;&gt;&lt;em&gt;Toki Pona &amp;ndash; The Language of Good&lt;/em&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ll post more on this at a later time&amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>History Repeats Itself</title>
      <link>https://ojmason.github.io/blog/history-repeats-itself/</link>
      <pubDate>Fri, 07 Apr 2017 11:11:30 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/history-repeats-itself/</guid>
      <description>&lt;p&gt;At the AAAI-84 conference, more than 30 years ago, was a panel discussion on &lt;em&gt;The Dark Ages of AI&lt;/em&gt;. Drew McDermott
said in the article in AI Magazine (vol 6 no 3 1985):&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;In spite of all the commercial hustle and bustle around AI these days there&amp;rsquo;s a mood [&amp;hellip;] of deep unease among
AI reserachers [&amp;hellip;]. This unease is due to the worry that that perhaps expectations about AI are too high,
and that this will eventually end in disaster.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;I have a real feeling of deja-vu. We have been there before, in the late 1960s, when the failed promises of
instant high-quality machine translation caused what was dubbed the &lt;em&gt;AI Winter&lt;/em&gt;, with all research funding
being stopped. AI recovered, and twenty years later the same happened. And now again.&lt;/p&gt;

&lt;p&gt;The situation has somewhat changed, as we have an abundance of data and processing power, but still no intelligence.
The algorithms are still the same old ones we had before, only on a larger scale.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_neural_network&#34;&gt;Artificial neural networks&lt;/a&gt; are faster,
and with &lt;a href=&#34;https://en.wikipedia.org/wiki/Deep_learning&#34;&gt;Deep Learning&lt;/a&gt; they have become easier to use. We are replacing human engineering work (feature selection etc)
by clever algorithms and are letting the machine find things out by itself.&lt;/p&gt;

&lt;p&gt;The problem is that it works fine in some situations, and then it suddenly becomes the magic bullet. Everybody now has
to do Deep Learning, as it has become the new marketing buzzword. AI is everywhere, but also nowhere, really. We do
not know anything more about the world than we did before, but we can get fast computers to recognise patterns in
data. &lt;a href=&#34;https://ojmason.github.io/blog/arguable-intelligence/&#34;&gt;No intelligence there&lt;/a&gt;. And if you are trying to calm down expectations you are not
&amp;lsquo;disruptive&amp;rsquo; enough.&lt;/p&gt;

&lt;p&gt;It is only a question of time until it all comes crashing down, and AI will be be blamed for empty promises. We&amp;rsquo;ve
been there before.&lt;/p&gt;

&lt;p&gt;This all fits into the overall pattern of human behaviour. In Europe we are forgetting the lessons from the early
20th century, and nationalism is on the rise; it might seem a bit over the top to compare the current situation to
1930s Germany, but the point is that at the time people didn&amp;rsquo;t know what was happening. We now have the benefit of
knowing where it can lead to, and it would be foolish to ignore the warning signs.&lt;/p&gt;

&lt;p&gt;In AI as in society: history always repeats itself, and if you forget the past, you are condemmed to repeat it.
(&lt;a href=&#34;https://en.wikipedia.org/wiki/George_Santayana&#34;&gt;George Santayana&lt;/a&gt;)&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Note: thanks to @DiegoKuonen for tweeting the reference to the AAAI-84 paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Arguable Intelligence</title>
      <link>https://ojmason.github.io/blog/arguable-intelligence/</link>
      <pubDate>Tue, 28 Mar 2017 16:45:58 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/arguable-intelligence/</guid>
      <description>&lt;p&gt;I work in the field of AI. I studied it at university, I read books about AI in my spare time,
implement toy systems that operate in the field, and I try to keep uptodate with current developments.
And it infuriates me&amp;hellip;&lt;/p&gt;

&lt;p&gt;Artificial Intelligence has moved from an academic/applied subject to a marketing buzzword. Every
washing machine is now &amp;ldquo;powered by AI&amp;rdquo;. So AI means everything and nothing these days. Let&amp;rsquo;s look at
the traditional definitions:&lt;/p&gt;

&lt;dl&gt;
&lt;dt&gt;Hard AI&lt;/dt&gt;
&lt;dd&gt;the cognitive processes of human beings are implemented in software. By doing so, a computer
basically functions like a human being, the brain being equivalent to a micro processor.&lt;/dd&gt;
&lt;dt&gt;Soft AI&lt;/dt&gt;
&lt;dd&gt;the cognitive system of a human being is imitated by computer software. There is no claim that
this models human cognition, but the outcomes are the same.&lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;This dichotomy was current when I learned about AI. But now there is a new element which arose in the
past couple of decades with the availability of large amounts of data in all sorts of domains: machine
learning (ML). In ML you basically train a mathematical model with data, which allows you to classify
other data items (which your system has not seen before). Or maybe map between one item (eg an English phrase)
and another (eg an equivalent German phrase). The difference here is that there is &lt;em&gt;no human cognition involved&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;AI is a direct reference to cognition, hence the &amp;lsquo;I&amp;rsquo; of it, but ML is not. It&amp;rsquo;s an automaton, which has no
understanding of what it is doing. In early AI systems, domain knowledge was usually modelled in a
representation system, a logical language, a semantic network, whatever.
&lt;a href=&#34;https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning&#34;&gt;Knowledge representation&lt;/a&gt; was/is an
important aspect of AI, as you cannot have AI without knowledge. In an ML system, knowledge is encoded in
model parameters, numerical values that have no meaning outside the model.&lt;/p&gt;

&lt;p&gt;One point that motivates me to work in AI is that we learn about how humans process the world. We find out what
is important, how me make decision, how we evaluate what is better or worse. In ML we just feed a black box with
examples that we have marked up as good or bad, and we get a black box that can distinguish between these categories
with a more or less acceptable accuracy. What have we learned about the process of making that distinction?&lt;/p&gt;

&lt;p&gt;Nothing.&lt;/p&gt;

&lt;p&gt;To re-iterate, machine learning is not artificial intelligence. It is statistical engineering. I&amp;rsquo;m not denying
that it is useful, but calling it AI is plain wrong, and will undoubtedly lead to a new
&lt;a href=&#34;https://en.wikipedia.org/wiki/AI_winter&#34;&gt;AI winter&lt;/a&gt; once the bubble bursts.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> Software is Sh*t</title>
      <link>https://ojmason.github.io/blog/software-is-sht/</link>
      <pubDate>Mon, 27 Mar 2017 15:00:26 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/software-is-sht/</guid>
      <description>

&lt;h1 id=&#34;software-is-sh-t&#34;&gt;Software is sh*t&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve been trying to get back into blogging, the easy way. A minimal, static website,
easy to publish stuff, no big overheads, control over what I write. I looked at
hosting pages on github, which seemed like a good enough option. Github pages suggest
something called Jekyll as a generator for the site &amp;ndash; much better than manually coding
everything up in HTML and linking stuff by hand.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a nightmare.&lt;/p&gt;

&lt;p&gt;Why, in 2017, is it harder to install some dinky piece of software than it was 20 years
ago? Why do I need to download half the packages on the internet to resolve dependencies?
And what do I do (as it invariably happens) when somthing falls over with a cryptic error
message? It&amp;rsquo;s enough to turn you into a grumpy old man.&lt;/p&gt;

&lt;p&gt;Then one of my twitter friends recommended Hugo. Installation was fine, only then github
was confused. It is already well known that &lt;code&gt;git&lt;/code&gt; is not exactly the pinnacle of usability,
and it doesn&amp;rsquo;t help that it seems to become the default tool for any command-line job. I can
see that a website can be modelled with a repository, and it&amp;rsquo;s probably a good idea, but
git seems such an overkill for 90% of what you would want to do.&lt;/p&gt;

&lt;p&gt;Several attempts and hours later I have a basic site running. But, no white space between the
date and title of the blog post. And no idea how to fix that. Gah. Maybe it would have been
better to just hand-code everything.&lt;/p&gt;

&lt;p&gt;It is understandable that website generation is complex. This can easily be seen by the number of
generators available for doing so. Everybody who tries to use one seems to find it complicated and
illogical, and then writes their own. Which slowly grows into a behemoth that is complicated and
illogical.&lt;/p&gt;

&lt;p&gt;It might sound arrogant and elitist, but I think software engineering is harder than many people
think. It&amp;rsquo;s always easier to start a new project than to maintain and curate an existing one. As
a logical consequence we have a proliferation of software projects, mostly doing the same things
in different ways. And few of them are of a high enough standard to be properly usable.&lt;/p&gt;

&lt;p&gt;I really wanted to end this post on a constructive note, but I don&amp;rsquo;t think I&amp;rsquo;m able to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> First Post</title>
      <link>https://ojmason.github.io/blog/first-post/</link>
      <pubDate>Mon, 27 Mar 2017 14:40:17 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/first-post/</guid>
      <description>

&lt;h1 id=&#34;first-post&#34;&gt;First Post&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m trying blogging again &amp;ndash; twitter is a bit too sparse for longer rants. And this seems
to be a time for rants. Lots happening in politics.&lt;/p&gt;

&lt;p&gt;Anyway, here we go again.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; I&amp;rsquo;m also going through my previous WordPress log to gather some articles that
might still be interesting, so there will be older articles going back almost a decade
appearing here.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solving Combinatory Problems with Erlang</title>
      <link>https://ojmason.github.io/blog/solving-combinatory-problems-with-erlang/</link>
      <pubDate>Sun, 10 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/solving-combinatory-problems-with-erlang/</guid>
      <description>&lt;p&gt;My daughter had some maths homework the other day:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You have 4 bags, each full of the numbers 1, 3, 5, and 7 respectively.
Take 10 of the numbers that when added up make 37. What numbers are they?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So far so good: that sounds easy enough. But a bit of trial and
error quickly leads nowhere. Something can’t be right. So, let’s
get the computer to work it out.&lt;/p&gt;

&lt;p&gt;As I haven’t done much Erlang recently I thought I’d give it a go.
And, during a casual glance at Armstrong’s &lt;em&gt;Programming in Erlang&lt;/em&gt; I
thought I’d finally understood list comprehensions, so I wrote the
following program:&lt;/p&gt;

&lt;pre&gt;
-module(comb).
-export([result/0]).
result() -&gt;
[{A+B+C+D+E+F+G+H+I+J,A,B,C,D,E,F,G,H,I,J}||
A &lt;- [1,3,5,7],
B &lt;- [1,3,5,7],
C &lt;- [1,3,5,7],
D &lt;- [1,3,5,7],
E &lt;- [1,3,5,7],
F &lt;- [1,3,5,7],
G &lt;- [1,3,5,7],
H &lt;- [1,3,5,7],
I &lt;- [1,3,5,7],
J &lt;- [1,3,5,7],
A+B+C+D+E+F+G+H+I+J =:= 37].
&lt;/pre&gt;

&lt;p&gt;I declare a module with one function, &lt;code&gt;result/0&lt;/code&gt;. This finds me ten
variables that can take any of the four specified values and add
up to 37. Simples!&lt;/p&gt;

&lt;p&gt;The list comprehension has ten generators, and one filter; it will
return a tuple with the sum and the individual variables’ values.&lt;/p&gt;

&lt;pre&gt;
Erlang R16B01 (erts-5.10.2) [64-bit] [smp:4:4] [async-threads:10] [hipe] [kernel-poll:false]
Eshell V5.10.2 (abort with ^G)
1&gt; comb:result().
[]
2&gt;
&lt;/pre&gt;

&lt;p&gt;WTF???! An empty list?! So I try changing the 37 to another value, like 36.&lt;/p&gt;

&lt;pre&gt;
3&gt; comb:result().
[{36,1,1,1,1,1,3,7,7,7,7},
{36,1,1,1,1,1,5,5,7,7,7},
{36,1,1,1,1,1,5,7,5,7,7},
{36,1,1,1,1,1,5,7,7,5,7},
{36,1,1,1,1,1,5,7,7,7,5},
[etc, etc].
&lt;/pre&gt;

&lt;p&gt;So it does work! Only, there doesn’t seem to be an answer to the
question. And with a bit of logical reasoning it is obvious: when
adding two odd numbers, you get an even number. So adding ten odd
numbers also yields an even number, but 37 is odd.&lt;/p&gt;

&lt;p&gt;What I learnt from this exercise: thinking about the problem
beforehand can save you time, as there was no need to write a program
at all. But then, I did get to use list comprehensions, and have
learnt how powerful they are. And it neatly shows Erlang’s Prolog
roots as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Party Politics and the Emperor&#39;s New Clothes</title>
      <link>https://ojmason.github.io/blog/party-politics-and-the-emperors-new-clothes/</link>
      <pubDate>Sat, 24 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/party-politics-and-the-emperors-new-clothes/</guid>
      <description>&lt;p&gt;On Twitter I came across a link to the (German) &lt;a href=&#34;http://www.cdu.de/sites/default/files/media/dokumente/regierungsprogramm-in-leichter-sprache-btw13.pdf&#34;&gt;CDU Wahlprogramm in “Leichter Sprache”&lt;/a&gt;.
This is the conservative party’s election manifesto in a German
version of &lt;a href=&#34;https://en.wikipedia.org/wiki/Basic_English&#34;&gt;Basic English&lt;/a&gt;,
to make it more easily comprehensible to
people who have difficulty understanding standard German.&lt;/p&gt;

&lt;p&gt;At first I thought it was a parody, as it sounded truly ridiculous
and patronising. As if you had to explain to your 4-year-old what
the world was like. Then I found out about the “&lt;a href=&#34;http://de.wikipedia.org/wiki/Leichte_Sprache&#34;&gt;Leichte Sprache&lt;/a&gt;”
(literally: “&lt;a href=&#34;http://en.wikipedia.org/wiki/Plain_language&#34;&gt;easy/plain language&lt;/a&gt;“), which I had not heard of before.
And in principle it is a noble thing to do, to make your manifesto
more accessible. I’m sure &lt;a href=&#34;https://www.mtholyoke.edu/acad/intrel/orwell46.htm&#34;&gt;George Orwell&lt;/a&gt; would approve!&lt;/p&gt;

&lt;p&gt;But the real reason why it does seem strange is rooted in the nature
of political language: normally, language is used to obfuscate and
manipulate, especially in complex areas such as politics. Fancy
words are used to disguise true intentions and get people to vote
for you. But take away the fancy words and complex phrases, and
suddenly all is out there in the open, plain to see for everybody.
And it turns out to be pretty meaningless. “Everybody should have
a good job. And earn enough money. That needs to be written down.”
But no mention about minimum wage or anything concrete to turn those
vacuous phrases into reality.&lt;/p&gt;

&lt;p&gt;Some other people on twitter reacted in the same way as I did; often
I guess out of the same ignorance of the “easy language” concept.
That’s a shame, but partly rooted in the patronising way the sentences
come across. But it also shows that the (undesired) outcome of this
publication is that people tend to not take the content seriously.
Because there is not much of it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Critical_discourse_analysis&#34;&gt;Critical Discourse Analysis&lt;/a&gt;
would lose a big part of its subject
area if politics would switch wholesale to “easy language”. No more
subconscious manipulation between the lines, no more obfuscation
about who does what to whom. So after some reflection I applaud the
CDU for making their manifesto available in this way, as it unmasks
them as the patronising right-wingers they are, with their overly
simplistic world view about pretty much any subject area in current
politics. No more hiding behind fancy words that foreigners are not
welcome. No more vague claims about surveillance cameras stopping
crime. It’s all there, in plain language. And other parties seem
to have done the same.&lt;/p&gt;

&lt;p&gt;Now the only thing that needs to go is the disclaimer at the
beginning: that this plain version is not the real manifesto, and
only the real one does count. I would prefer it to be the other way
round.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Every Test is Valuable</title>
      <link>https://ojmason.github.io/blog/every-test-is-valuable/</link>
      <pubDate>Tue, 17 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/every-test-is-valuable/</guid>
      <description>&lt;p&gt;After reading Graham Lee’s &lt;a href=&#34;http://www.amazon.co.uk/gp/product/B007RNK0W6/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=1634&amp;amp;creative=19450&amp;amp;creativeASIN=B007RNK0W6&amp;amp;linkCode=as2&amp;amp;tag=phrasysnlp-21&#34;&gt;Test-Driven iOS Development&lt;/a&gt;; (disclaimer:
affiliate link) I have (again) adopted a test-driven approach to
software developing. Whenever I create a new class I write a bunch
of tests exercising the class’ properties. One might question the
value of this, because there is not really any reason why those
should not work. However, having such a test in place just uncovered
an as-yet unnoticed bug I introduced in a project.&lt;/p&gt;

&lt;p&gt;Originally the class property in question was going to be set in
the &lt;code&gt;init&lt;/code&gt; method, so I tested for the presence of the property
after creating an instance of the relevant class. Easy pass. Weeks
later I did something (and forgot to run the test suite). Today I
did something else, and this time I did run them. Hey presto, failed
test. And completely unexpected, because it was the property
exercising one. How on Earth did that happen?&lt;/p&gt;

&lt;p&gt;Upon closer inspection I tracked it down to a refactoring, where I
extracted some code from &lt;code&gt;init&lt;/code&gt; as there were now multiple ways an
object could be initialised. The failing code was part of that, and
I realised that it was called from the new &lt;code&gt;initFromFile:&lt;/code&gt; method,
but &lt;em&gt;no longer from the default initialiser&lt;/em&gt;. Easy mistake to make.
And had I run my test-suite more consistently, my application would
not have been with a potential bug in the meantime.&lt;/p&gt;

&lt;p&gt;What did I take home from this?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Even Mickey-Mouse-tests are valuable. No test is too small to be useful
(provided it’s actually testing something valid).&lt;/li&gt;
&lt;li&gt;The test-suite should really be run after every change. I’ll have to check
if I can get Xcode to run them automatically after each compilation…&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Update&lt;/em&gt;: Since I stopped doing much iOS or Mac OS work, I no longer used Xcode.
And I&amp;rsquo;m not missing it :)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Numerical Illiteracy</title>
      <link>https://ojmason.github.io/blog/numerical-illiteracy/</link>
      <pubDate>Sun, 30 Jan 2011 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/numerical-illiteracy/</guid>
      <description>&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; This is an older article from the blog I ran while being an academic. Most of the posts
on there are about teaching and learning, but this one is of more general relevance, see also
&lt;a href=&#34;https://ojmason.github.io/longest-chip-in-the-world/&#34;&gt;this one&lt;/a&gt;.
&lt;hr&gt;
I had a look at our free local newspaper, the &lt;em&gt;Birmingham Mail Extra&lt;/em&gt;.
In this issue is an article titled &amp;ldquo;Suburb is now crime hotspot&amp;rdquo;.
This article has several problems, which are indicative of the
problems of representing what happens &amp;lsquo;out there&amp;rsquo; in the form of a
newspaper article.&lt;/p&gt;

&lt;p&gt;Quinton had very little burglaries, &amp;ldquo;averaging just one or two
burglaries a month&amp;rdquo; over the last nine years. But now, crime has
&amp;ldquo;shot up by 29%&amp;rdquo;, more than the rest of Birmingham where crime only
rose by 21%.&lt;/p&gt;

&lt;p&gt;I find this rather misleading, and I worked through an example with
the kids at the dinner table, which illustrates the problem:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Suburb A had 10 burglaries in 2009, and 12 in 2010. That is an increase of 20%.&lt;/li&gt;
&lt;li&gt;Suburb B had 4 burglaries in 2009, and 5 in 2010. Increase of 25%.&lt;/li&gt;
&lt;li&gt;Suburb C had 1 burglary in 2009, and 2 in 2010. A whopping 100% increase.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;First lesson:&lt;/em&gt; percentage increase is meaningless unless everybody
started in the same place. Why do developing economies have higher
growth rates than Europe and the US? Because the percentage looks
bigger. If you double your output from 100 cars to 200 cars you
increase by 100%, but to maintain that rate you will have to produce
an additional 200 cars the year after, and then 400, 800 etc.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Second lesson:&lt;/em&gt; without a fixed reference point (such as burglaries
per 1000 inhabitants) you have no idea whether the risk of your
house being burgled is high or low. Everything is relative, but you
still need a fixed point to evaluate things.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Third lesson:&lt;/em&gt; this is not mentioned, but is that difference between
29% and 21% statistically significant? With &amp;ldquo;one or two burglaries
a month&amp;rdquo;, the variation seems rather high, given those small numbers.
So if there was one extra or one fewer burglaries, how would the
29% change?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Final lesson:&lt;/em&gt; be careful with calling places &amp;ldquo;crime hotspot&amp;rdquo; when
your statistics are that shaky.&lt;/p&gt;

&lt;p&gt;Now I don&amp;rsquo;t expect a staff reporter at the &lt;em&gt;Birmingham Mail&lt;/em&gt; to apply
the same rigour as a scientist (or any other, non-science researcher),
but the way the article is presented is simply misleading. To me
this sounds like a scare story being created out of some random
statistics. I don&amp;rsquo;t know whether that is the case, and maybe Quinton
is really a crime-ridden area, but I cannot tell from the few facts
given to me in the article. The reason for this development given
in the paper is that there are now 5 fewer policemen covering
Quinton. Is there really such a cast-iron correlation between
policemen and crime rate? How about the influence of the recession
on crime? And how many policemen were there before the cut? 50? 500?
5000?&lt;/p&gt;

&lt;p&gt;I now have more questions and know less than I did before reading
that article!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The longest Chip in the World</title>
      <link>https://ojmason.github.io/blog/the-longest-chip-in-the-world/</link>
      <pubDate>Sat, 18 Dec 2010 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/the-longest-chip-in-the-world/</guid>
      <description>&lt;p&gt;In his &lt;a href=&#34;http://www.amazon.co.uk/gp/product/0465045669?ie=UTF8&amp;amp;tag=phrasysnlp-21&amp;amp;linkCode=as2&amp;amp;camp=1634&amp;amp;creative=19450&amp;amp;creativeASIN=0465045669&#34;&gt;Metamagical Themas: Questing for the Essence of Mind and
Pattern&lt;/a&gt;,
Douglas Hofstadter talks about &lt;em&gt;numerical literacy&lt;/em&gt;, the
ability to understand large numbers. This is especially important
when state budgets are thrown around which deal with billions of
pounds or euros. At some point you just lose all feeling for
quantities, as they are all so unimaginably large and abstract. He
then goes on to pose some rough calculation questions, such as “How
many cigarettes are smoked in the US every day?” – you start with
some estimates, and then work out an answer, which might be near
the order of magnitude of the right answer (which we of course don’t
know). Quite an interesting and useful mental exercise.&lt;/p&gt;

&lt;p&gt;Yesterday we had Fish &amp;amp; Chips for dinner. The girls were comparing
the sizes of their chips, and then we came on to the topic of the
longest chip in the world. Obviously, with ‘proper’ chips this is
limited by the &lt;a href=&#34;http://www.france-today.com/2009/12/longest-chip-in-world-isof-course.html&#34;&gt;size of the source potato&lt;/a&gt;.
However, assuming that industrially
produced chips are made of mashed potato formed into chip-shapes,
there is not really any fixed limit on the length. So, thinking
of Hofstadter, I asked them how many potatoes we would need to make
a chip that spans around the whole world.&lt;/p&gt;

&lt;p&gt;Rough assumptions: one potato contains enough matter to produce
10cm worth of chip (the thickness is not specified). So, how many
potatoes do we need for one metre of chip? This is also useful to
practice basic primary-school-level maths… – 10. How many for a
kilometre? 10,000. How many kilometres do we need to span the world?
Roughly 40,000 km. So how many potatoes do we need? 400 million.&lt;/p&gt;

&lt;p&gt;The next question is whether there are enough potatoes in the world
to do this. Assuming a potato weighs 100g, how much do our 400
million potatoes weigh? 40 million kilogrammes, or 40,000 (metric)
tons. What is the world’s potato production? According to &lt;a href=&#34;http://en.wikipedia.org/wiki/Potato&#34;&gt;Wikipedia&lt;/a&gt;,
this is 315 million metric tons, so plenty enough. Now, if we were
to turn the annual potato crop into one long chip, how many times
would it go round the Earth? 7,875 times.&lt;/p&gt;

&lt;p&gt;So, with a bit of basic maths (and Wikipedia for the data) you can
make maths exciting for kids, practice how to multiply and divide,
teach problem-solving, and have fun at the same time. And they also
get a feeling for numbers: 400 million – that’s how many potatoes
you need for a chip to span the Earth.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentence Disambiguation -- Modality to the Rescue!</title>
      <link>https://ojmason.github.io/blog/sentence-disambiguation----modality-to-the-rescue/</link>
      <pubDate>Thu, 12 Nov 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/sentence-disambiguation----modality-to-the-rescue/</guid>
      <description>&lt;p&gt;I’m currently reading a new book on iPhone development, &lt;a href=&#34;http://www.amazon.co.uk/gp/product/1430224037?ie=UTF8&amp;amp;tag=phrasysnlp-21&amp;amp;linkCode=as2&amp;amp;camp=1634&amp;amp;creative=6738&amp;amp;creativeASIN=1430224037&#34;&gt;&lt;em&gt;iPhone
Advanced Projects&lt;/em&gt;&lt;/a&gt;
 by Apress. I will probably talk about that book
in a later post, but today I will just focus on one sentence I came
across on page 212:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I also adore the capability that I have to flag articles from folks I follow on Twitter and save them to Instapaper.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This sentence has (at least) two readings, which are probably only
obvious to a linguist (and who else would care?); I highlight the
differences by adding commas:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I adore the capability, that I have to flag articles…&lt;/li&gt;
&lt;li&gt;I adore the capability that I have, to flag articles…&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the first case you adore the capability. And the capability is
that you have to do something (flag articles). Sounds rather odd,
doesn’t it? The second case is more clear-cut and easy to understand:
you can flag articles, and that’s the capability you have and adore.&lt;/p&gt;

&lt;p&gt;So in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/Pattern_grammar&#34;&gt;pattern grammar&lt;/a&gt;,
you’re either looking at &lt;em&gt;N that&lt;/em&gt; or
&lt;em&gt;N &lt;em&gt;to&lt;/em&gt;-inf&lt;/em&gt; with &lt;em&gt;capability&lt;/em&gt;. If you consult the &lt;a href=&#34;http://www.amazon.co.uk/gp/product/1424008255?ie=UTF8&amp;amp;tag=phrasysnlp-21&amp;amp;linkCode=as2&amp;amp;camp=1634&amp;amp;creative=6738&amp;amp;creativeASIN=1424008255&#34;&gt;Cobuild Dictionary&lt;/a&gt;,
you’ll find that &lt;em&gt;capability&lt;/em&gt; only occurs with the second pattern,
the &lt;em&gt;to&lt;/em&gt;-infinitive, so that you can rule out the first reading.&lt;/p&gt;

&lt;p&gt;Another possibility would be to look at it in terms of &lt;a href=&#34;http://en.wikipedia.org/wiki/English_modal_auxiliary_verb&#34;&gt;modality&lt;/a&gt;:
here we could argue that &lt;em&gt;capability&lt;/em&gt; prospects a modality of ability,
but &lt;em&gt;have to&lt;/em&gt; expresses obligation; the two don’t go together. Hence
the first reading sounds odd, as a capability does not usually force
you to do anything, but rather enables you. It could, however, be
used to signal sarcasm or irony, as in (the obviously made up) &lt;em&gt;I
really like that my new computer gives me the capability to have
to save my work every five minutes&lt;/em&gt;. This is clearly an odd sentence,
suggesting that modality works along similar lines as &lt;a href=&#34;http://en.wikipedia.org/wiki/Discourse_prosody&#34;&gt;discourse
prosody&lt;/a&gt; as described by Louw (1993) [for the full reference follow
the previous link].&lt;/p&gt;

&lt;p&gt;Here we have discussed two ways of disambiguating a sentence, one
based on grammatical properties (or typical environments), and one
on a non-syntactic phenomenon (modality). Pattern grammar allows
us to identify what the typical usage would be, whereas modality
explains to us why the first reading is at odds with the corresponding
words. Now all we need is a ‘pattern grammar’ for modality!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collocations -- Do we need them?</title>
      <link>https://ojmason.github.io/blog/collocations----do-we-need-them/</link>
      <pubDate>Tue, 03 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/collocations----do-we-need-them/</guid>
      <description>

&lt;p&gt;The concept of collocation was introduced in the middle of the last
century by J.R. Firth with his famous quote “You shall know a word
by the company it keeps”. Words are not distributed randomly in a
text, but instead they stick with each other, their ‘company’.
Starting in the late 1980s, the increased interest in collocation
by computational linguists and others working in NLP has lead to a
proliferation of methods and algorithms to extract collocations
from text corpora.&lt;/p&gt;

&lt;p&gt;Typically one starts with the environment of the target (or node)
word, and collects all the words that are within a certain distance
(or span) of the node. Then their frequency in a reference corpus
is compared with their frequency in the environment of the node,
and from the ratio of frequencies we determine whether they’re near
the node by chance or because they’re part of the node’s company.
A bewildering variety of so-called significance functions exists,
the oldest probably being the &lt;em&gt;z&lt;/em&gt;-score, used by Berry-Rogghe in 1973;
later, Church and Hanks (1991) popularised mutual information and
&lt;em&gt;t&lt;/em&gt;-score, which now seem to have been displaced by log-likelihood
as the predominant measure of word association.&lt;/p&gt;

&lt;p&gt;The problem is: all these metrics yield different results, and
nobody knows (or can tell) which are ‘right’. Mutual information,
for example, favours rare words, while the &lt;em&gt;t&lt;/em&gt;-score promotes words
which are relatively frequent already. But apart from rules-of-thumb,
there exists no linguistic justification why one metric is preferable
to another. It is all rather ad-hoc.&lt;/p&gt;

&lt;p&gt;Part of this is that collocation as a concept is rather underspecified.
What does it mean for a word to be ‘significantly more common’ near
the node word as opposed to be there just by chance? In a sense,
collocations are just diagnostics: we know there are words that are
to be expected next to &lt;em&gt;bacon&lt;/em&gt;, and we look for collocates and find
&lt;em&gt;rasher&lt;/em&gt;. Fantastic! Just what we expected. But then we look at &lt;em&gt;fire&lt;/em&gt;,
and find &lt;em&gt;leafcutter&lt;/em&gt; as a very significant collocate. How can that
happen? What is the connection between &lt;em&gt;fire&lt;/em&gt; and &lt;em&gt;leafcutter&lt;/em&gt;? The
answer is: &lt;em&gt;ants&lt;/em&gt;. There are &lt;em&gt;fire ants&lt;/em&gt;, and there are &lt;em&gt;leafcutter ants&lt;/em&gt;,
and they are sometimes mentioned in the same sentence.&lt;/p&gt;

&lt;p&gt;This leads us to an issue which I believe gets us on the right track
in the end: the fallacy of using the word as the primary unit of
analysis. In the latter example, we’re not dealing with &lt;em&gt;fire&lt;/em&gt; and
&lt;em&gt;leafcutter&lt;/em&gt;, we’re instead concerned with &lt;em&gt;fire ants&lt;/em&gt;. Once we realise
that, then it is perfectly natural to see &lt;em&gt;leafcutter ants&lt;/em&gt; as a
collocate, whereas we would be surprised to find &lt;em&gt;engine&lt;/em&gt;, which
instead is a collocate of the lexical item &lt;em&gt;fire&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So, phraseology is the clue. If we get away from single words, and
instead consider multi-word units, then we also have an explanation
for collocations. Single words form part of larger MWUs, together
with other single words. So &lt;em&gt;leafcutter&lt;/em&gt; often forms a unit with &lt;em&gt;ants&lt;/em&gt;,
as does &lt;em&gt;fire&lt;/em&gt;. More generally, MWUs such as &lt;em&gt;parameters of the model&lt;/em&gt;
are formed of several single words, and here we can observe that
&lt;em&gt;parameters&lt;/em&gt; and &lt;em&gt;model&lt;/em&gt; occur together. But they form a single unit
of analysis, and only if we break up this unit by considering single
words, then we can observe that &lt;em&gt;parameters&lt;/em&gt; and &lt;em&gt;model&lt;/em&gt; commonly occur
together.&lt;/p&gt;

&lt;p&gt;From this we can define a very simple procedure to compute collocations:
from a corpus, gather all the MWUs that are associated with a
particular word. Get a frequency list of all the single word items
in those MWUs, sort by frequency, and there we are.&lt;/p&gt;

&lt;p&gt;To conclude, &lt;em&gt;collocation is an epiphenomenon of phraseology&lt;/em&gt;, a
side-effect of words forming larger units. Phraseological units
contain multiple single words, and those are picked up by collocation
software, because those are the ones that commonly occur in a text
together. And the reason for occurring together is that they form
a single unit. Once we look at text in terms of MWUs, the need for
collocation disappears. Collocation just picks out the constituent
elements of multi-word units.&lt;/p&gt;

&lt;p&gt;One could of course argue that this is a circular argument, that
we are simply replacing a procedure to calculate collocations by
one that calculates MWUs. But the difference between those two
procedures is that MWU-recognition does not require complicated
statistics (which I find hard to see justification for), but instead
simply looks at recurrent patternings in language. MWUs are re-usable
chunks of texts, which can be justified on the grounds of usage.
Collocation is a much harder concept to explain and integrate into
views of language. And, as it turns out, we don’t really need it
at all.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;dl&gt;
&lt;dt&gt;Berry-Rogghe, G.L.M. (1973)&lt;/dt&gt;
&lt;dd&gt;“The Computation of Collocations and Their Relevance in Lexical Studies.” in The Computer and Literary Studies. Eds. A.J. Aitken, R.W. Bailey and N. Hamilton-Smith. Edinburgh: Edinburgh University Press, p 103-112.&lt;/dd&gt;
&lt;dt&gt;Church, K., and Hanks, P. (1991)&lt;/dt&gt;
&lt;dd&gt;“Word Association Norms, Mutual Information and Lexicography,” Computational Linguistics, Vol 16:1, p 22-29.&lt;/dd&gt;
&lt;dt&gt;Firth, J. R. (1957)&lt;/dt&gt;
&lt;dd&gt;“A Synopsis of Linguistic Theory 1930-1955” in Studies in Linguistic Analysis, Oxford: Philological Society.&lt;/dd&gt;
&lt;/dl&gt;
</description>
    </item>
    
    <item>
      <title>Thinking Erlang, or Creating a Random Matrix without Loops</title>
      <link>https://ojmason.github.io/blog/thinking-erlang-or-creating-a-random-matrix-without-loops/</link>
      <pubDate>Thu, 26 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/thinking-erlang-or-creating-a-random-matrix-without-loops/</guid>
      <description>&lt;p&gt;For a project, my &lt;a href=&#34;https://ojmason.github.io/blog/fast-pfnets-in-erlang/&#34;&gt;Erlang implementation of a fast PFNET algorithm&lt;/a&gt;,
I needed to find a way to create a random matrix of integers (for
path weights), with the diagonal being filled with zeroes.  I was
wondering how best to do that, and started off with two loops, an
inner one for each row, and an outer one for the full set of rows.
Then the problem was how to tell the inner loop at what position
the ‘0’ should be inserted.  I was thinking about passing a row-ID,
when it suddenly clicked: &lt;code&gt;lists:seq/2&lt;/code&gt; was what I needed!  This
method, which I previously thought was pretty useless, creates a
list with a sequence of numbers (the range is specified in the two
parameters).  For example,&lt;/p&gt;

&lt;pre&gt;
1&gt; lists:seq(1,4).
[1,2,3,4]
2&gt; lists:seq(634,637).    
[634,635,636,637]
3&gt; lists:seq(1000,1003).
[1000,1001,1002,1003]
&lt;/pre&gt;

&lt;p&gt;Now I would simply generate a list with a number for each row, and
then send the inner loop off to do its thing, filling the slot given
by the sequence number with a zero, and others with a random value.&lt;/p&gt;

&lt;p&gt;But now it gets even better.  Using a separate (tail-)recursive
function for the inner loop didn’t quite seem right, so I thought
a bit more about it and came to the conclusion that this is simply
a mapping; mapping an integer to a list (a vector of numbers, one
of which (given by the integer) is a zero).  So instead of using a
function for filling the row, I call &lt;code&gt;lists:seq/2&lt;/code&gt; again and then map
the whole thing.  This is the final version I arrived at, and I’m
sure it can still be improved upon using list comprehensions:&lt;/p&gt;

&lt;pre&gt;
random_matrix(Size, MaxVal) -&gt;
  random:seed(),
  lists:map(
    fun(X) -&gt;
      lists:map(
          fun(Y) -&gt;
              case Y of 
                 X -&gt; 0; 
                 _ -&gt; random:uniform(MaxVal)
                 end
              end,
          lists:seq(1,Size))
      end,
    lists:seq(1,Size)).
&lt;/pre&gt;

&lt;p&gt;This solution seems to be far more idiomatic, and I am beginning
to think that I finally no longer think in an imperative way of
loops, but more in the Erlang-way of list operations.  Initially
this is hard to achieve, but with any luck it will become a lot
easier once one is used to it.  Elegance, here I come!&lt;/p&gt;

&lt;p&gt;Example run:&lt;/p&gt;

&lt;pre&gt;
4&gt; random_matrix(6,7).  
[[0,1,4,6,7,4],
 [3,0,5,7,5,4],
 [5,1,0,2,5,2],
 [4,2,4,0,3,1],
 [4,4,3,3,0,1],
 [5,7,3,2,2,0]]
&lt;/pre&gt;

&lt;p&gt;Note: I have used &lt;code&gt;random:seed/0&lt;/code&gt; above, as I am happy for the function
to return identical matrices on subsequent runs with the same
parameters. To get truly random results, that would have to be left
out. However, for my benchmarking purposes it saved me having to
save the matrix to a file and read it in, as I can easily generate
a new copy of the same matrix I used before.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fast PFNETs in Erlang</title>
      <link>https://ojmason.github.io/blog/fast-pfnets-in-erlang/</link>
      <pubDate>Sat, 14 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/fast-pfnets-in-erlang/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Pathfinder_Networks&#34;&gt;Pathfinder Networks&lt;/a&gt;
(PFNETs) are networks derived from a graph
representing proximity data.  Basically, each node is connected to
(almost) every other node by a weighted link, and that makes it
hard to see what’s going on.  The Pathfinder algorithm prunes the
graph by removing links which are weighted higher than another path
between the same nodes.&lt;/p&gt;

&lt;p&gt;For example: A links to B with weight 5.  A also links to C with
weight 2, and C links to B with weight 2.  Adding up the weights,
A to B direct is 5, A to C to B is 4.  Result: we remove the link
from A to B, as the route via C is shorter.  There are different
ways to calculate the path lengths (using the &lt;a href=&#34;http://en.wikipedia.org/wiki/Minkowski_metric&#34;&gt;Minkowski &lt;em&gt;r&lt;/em&gt;-metric&lt;/a&gt;),
but you get the general idea.  The resulting PFNET has fewer links
and is easier to analyse.&lt;/p&gt;

&lt;p&gt;In Schvaneveldt (1990) an algorithm for computing PFNETs is given,
but it is rather complex and computationally intensive.  There is
an improved algorithm called Binary Pathfinder, but that is apparently
more memory intensive.  Not very promising so far, but then along
comes &lt;em&gt;A new variant of the Pathfinder algorithm to generate large
visual science maps in cubic time&lt;/em&gt;, by Quirin, Cordón, Santamaría,
Vargas-Quesada, and Moya-Anegón.  This algorithm is a lot faster
(by about 450 times), but has one disadvantage: speed is traded in
for flexibility.  The original Pathfinder algorithm has two parameters,
&lt;em&gt;r&lt;/em&gt; (the value of the Minkowski metric to be used) and &lt;em&gt;q&lt;/em&gt; (the maximum
length of paths to be considered).  The fast algorithm only has &lt;em&gt;r&lt;/em&gt;,
and always uses the maximum value for &lt;em&gt;q&lt;/em&gt; (which is n-1).  I know too
little about the application of PFNETs to say whether this is
important at all; for the uses I can envisage it does not seem to
matter.&lt;/p&gt;

&lt;p&gt;As added bonus, the algorithm in pseudo code in Quirin et al. is
very short and straightforward.  They’re using a different &lt;a href=&#34;http://en.wikipedia.org/wiki/Floyd-Warshall_algorithm&#34;&gt;shortest-path
algorithm&lt;/a&gt;
to identify, erm, shorter paths.  And then it’s very
simple to prune the original network.&lt;/p&gt;

&lt;p&gt;A picture tells more than 1K words, so here instead of 2000 words
the before and after, graph layout courtesy of the graphviz program:&lt;/p&gt;

&lt;p&gt;A network example (from Schvaneveldt 1990)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ojmason.github.io/img/pfnets-1.png&#34; caption=&#34;Fig 1: A network example (from Schvaneveldt 1990)&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Here, each node is linked to every other node.  Running the PFNET
algorithm on it, we get the output shown in the second figure.&lt;/p&gt;

&lt;p&gt;A PFNET generated from the previous graph&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ojmason.github.io/img/pfnets-2.png&#34; caption=&#34;Fig 2: A PFNET generated from the previous graph&#34;/&gt;&lt;/p&gt;

&lt;p&gt;If you compare the output with the actual result from Schvaneveldt’s
book (p.6 / 7), you’ll realise that it is not identical, and the
reason for that is that the example there limits the path-length,
using the parameters (&lt;em&gt;r&lt;/em&gt; = 1, &lt;em&gt;q&lt;/em&gt; = 2) rather than (&lt;em&gt;r&lt;/em&gt; = 1, &lt;em&gt;q&lt;/em&gt; = n-1)
as in the example shown here.  As a consequence, the link from N1
to N4 (with a weight of 5) disappears, because of the shorter path
(N1-N2-N3-N4, weight 4).  But that path is too long if &lt;em&gt;q&lt;/em&gt; is just
2, and so it is kept in Schvaneveldt’s example.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;It is not possible to implement the pseudo-code given in Quirin &lt;em&gt;et
al&lt;/em&gt; directly (in Erlang), as they use destructive updates of the link matrix,
which we obviously cannot do in Erlang.  But the first, naïve,
implementation is still quite short.  The input graph is represented
as a matrix (&lt;em&gt;n&lt;/em&gt; x &lt;em&gt;n&lt;/em&gt;) where each value stands for the link weight,
with zero being used to indicate non-existing links.  I have written
a function that creates a dot file from a matrix, which is then fed
into graphviz for generating images as the ones shown above.&lt;/p&gt;

&lt;p&gt;There are basically two steps: creating a matrix of shortest paths
from the input matrix, and then generating the PFNET by comparing
the two matrices; if a certain cell has the same value in both
matrices, then it is a shortest path and is kept, otherwise there
is a shorter path and it’s pruned. Here is the main function:&lt;/p&gt;

&lt;pre&gt;
find_path(Matrix) -&gt;
    Shortest = loop1(1,length(Matrix),Matrix),
    generate_pfnet(Matrix, Shortest, []).
&lt;/pre&gt;

&lt;p&gt;Next we have the three loops (hence ‘cubic time’!) of the Floyd-Warshall
shortest path algorithm to create the shortest path matrix:&lt;/p&gt;

&lt;pre&gt;
loop1(K,N,Matrix) when K &gt; N -&gt; 
    Matrix;
loop1(K,N,Matrix) -&gt;
    NewMatrix = loop2(K,1,Matrix,
        Matrix,[]),
    loop1(K+1,N,NewMatrix).

loop2(_,_,_,[],Acc) -&gt;
    lists:reverse(Acc);
loop2(K,I,D,[Row|Rest],Acc) -&gt;
    NewRow = loop3(K,I,1,D, Row, []),
    loop2(K,I+1,D,Rest,[NewRow|Acc]).

loop3(_,_,_,_, [], Acc) -&gt;
    lists:reverse(Acc);
loop3(K,I,J,D, [X|Rest], Acc) -&gt;
    loop3(K,I,J+1,D, Rest,
    [min(X, get(I,K,D) + get(K,J,D))|Acc]).
&lt;/pre&gt;

&lt;p&gt;The final line implements the Minkowski metric with &lt;em&gt;r&lt;/em&gt; = 1; this
could be expanded to include other values as well, eg &lt;em&gt;r&lt;/em&gt; = 2 for
Euclidean, or &lt;em&gt;r&lt;/em&gt; = ∞ (which seem to be the most common values in
use; the latter means using the maximum weight of any path component
along the full path).&lt;/p&gt;

&lt;p&gt;And here are two utility methods, one to find the smaller of two
values, and one to retrieve an element from a matrix (which is a
list of rows).  There is something of a hack to deal with the fact
that zero does not mean a very small weight, but refers to a
non-existing link:&lt;/p&gt;

&lt;pre&gt;
min(X, Y) when X &lt; Y -&gt; X;
min(_X, Y) -&gt; Y.

get(Row, Col, Matrix) -&gt;
    case lists:nth(Col,
      lists:nth(Row, Matrix)) of
        0 -&gt; 999999999;
        X -&gt; X
    end.
&lt;/pre&gt;

&lt;p&gt;And finally, generating the PFNET by comparing the two matrices
(again, awful hack included):&lt;/p&gt;

&lt;pre&gt;
generate_pfnet([],[],Result) -&gt;
    lists:reverse(Result);
generate_pfnet([R1|Rest1], [R2|Rest2], Acc) -&gt;
    Row = generate_pfrow(R1,R2,[]),
    generate_pfnet(Rest1, Rest2, [Row|Acc]).

generate_pfrow([],[],Result) -&gt;
    lists:reverse(Result);
generate_pfrow([C|Rest1], [C|Rest2], Acc) -&gt;
    case C of
        999999999 -&gt; C1 = 0;
        _ -&gt; C1 = C
    end,
    generate_pfrow(Rest1, Rest2, [C1|Acc]);
generate_pfrow([C1|Rest1], [C2|Rest2], Acc) -&gt;
    generate_pfrow(Rest1, Rest2, [0|Acc]).
&lt;/pre&gt;

&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;So this is the basic code.  It works, but there is scope for improvement.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it only currently generates PFNETs with (&lt;em&gt;r&lt;/em&gt; = 1, &lt;em&gt;q&lt;/em&gt; = &lt;em&gt;n&lt;/em&gt;-1)&lt;/li&gt;
&lt;li&gt;there is no parallelism, hence it’s not really making use of Erlang’s strengths.&lt;/li&gt;
&lt;li&gt;the three loops don’t look very elegant, and could probably be replaced by list comprehensions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because of the matrix being updated, it doesn’t look that easy to
parallelise the processing, but it would work at the level of
updating the individual rows.  If that can be done in parallel, it
would probably provide some speed-up (provided the matrix is not
of a trivial size).  So the first step would be to change the matrix
processing using &lt;code&gt;lists:map/2&lt;/code&gt;, and replacing this then by &lt;code&gt;pmap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once the code is up to scratch with full parallelism, and tested
on larger matrices I will probably put it up on Google Code in case
other people are interested in using it.  If you have any suggestions,
tell me in the comments!&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;dl&gt;
&lt;dt&gt;Schvaneveldt, R. W. (Ed.) (1990)&lt;/dt&gt;
&lt;dd&gt;Pathfinder Associative Networks: Studies in Knowledge Organization. Norwood, NJ: Ablex.
The book is out of print. A copy can be downloaded from the &lt;a href=&#34;http://en.wikipedia.org/wiki/Pathfinder_Networks&#34;&gt;Wikipedia page&lt;/a&gt;&lt;/dd&gt;
&lt;dt&gt;Quirin, A; Cordón, O; Santamaría, J; Vargas-Quesada, B; Moya-Anegón, F (2008)&lt;/dt&gt;
&lt;dd&gt;“A new variant of the Pathfinder algorithm to generate large visual science maps in cubic time”, Information Processing and Management, 44, p.1611-1623.&lt;/dd&gt;
&lt;/dl&gt;
</description>
    </item>
    
    <item>
      <title>Kids and Computers</title>
      <link>https://ojmason.github.io/blog/kids-and-computers/</link>
      <pubDate>Tue, 08 Jul 2008 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/kids-and-computers/</guid>
      <description>&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; This did not turn out as I hoped. The main reason being that an old ZX Spectrum
pre-dates the common availability of the Internet, and as soon as it was switched on the
question arose, how one could watch YouTube videos on this machine. So, abject failure.
Times have moved on, kids these days don&amp;rsquo;t know they&amp;rsquo;re born, good old days, it was so much
better when I was young, spoilt kids nowadays etc etc etc.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Like many(?) computer people of my generation I started off with
the Sinclair ZX81, and then continued with the ZX Spectrum and
onwards to an Amstrad CPC, learning BASIC and Z80 machine code
(because those machines were actually pretty slow).  This also means
that I treat computers as tools, not passively using programs other
people have written, but writing my own stuff.  In time I moved on
with languages, from Pascal to C, a bit of Prolog, C++, Java, and
now Erlang.  But those endless hours typing in program listings in
BASIC have surely benefitted me.  Learning by doing.  Type in small
programs, change the code, observe what happens.&lt;/p&gt;

&lt;p&gt;My oldest daughter is seven, still half my age when I started with
computers, but from next year onwards they have ICT classes at
school; these basically seem to be ‘how to change fonts in MS Word’.
So I thought I’d better show her that a computer program is something
you write, not just something you use.  The only problem was, how
to do it?  Despite all its faults, BASIC seems to me to be a good
language to start with.  True, it has few control structures or
abstract data types, but it reads almost like English, and is only
a starting point after all.  And it’s only a first step in any case.&lt;/p&gt;

&lt;p&gt;The next question is how to get her interested.  Using a PC seems
like overkill, and you need to boot it up, and shut it down, and
then start a development environment etc.  And I guess it’s not
that easy to use graphics or sound as it was on the old Spectrum.
So, I went to ebay and bought a ZX Spectrum +2 for about £7.  You
plug it in, it’s on.  You can start typing.  You’re finished, you
unplug it.  No files to get corrupted.  And it’s got a built-in
tape recoder for storage.&lt;/p&gt;

&lt;p&gt;And, she is really excited.  It’s her very own computer, and she
already knows several commands.  All I need now is a few books with
BASIC programs, so that she has some material to type in.  In the
meantime I go with her through the manual, explaining the new
keywords, and she can already understand how a program is executed.
With any luck she’ll get hooked, especially once she learns how to
do graphics with it.  And the little games (‘guess the number’) are
also the right level for her, and maybe even her sisters.&lt;/p&gt;

&lt;p&gt;The only problem I can see is printing.  But I’ve got an emulator
on my laptop, which might be able to read in something she saves,
and I might be able to take it from there.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>