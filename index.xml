<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Arguable Intelligence</title>
    <link>https://ojmason.github.io/index.xml</link>
    <description>Recent content on Arguable Intelligence</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-gb</language>
    <lastBuildDate>Mon, 27 Mar 2017 15:00:26 +0100</lastBuildDate>
    <atom:link href="https://ojmason.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title> Software is Sh*t</title>
      <link>https://ojmason.github.io/blog/software-is-sht/</link>
      <pubDate>Mon, 27 Mar 2017 15:00:26 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/software-is-sht/</guid>
      <description>

&lt;h1 id=&#34;software-is-sh-t&#34;&gt;Software is sh*t&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;ve been trying to get into blogging, the easy way. A minimal, static website,
easy to publish stuff, no big overheads, control over what I write. I looked at
hosting pages on github, which seemed like a good enough option. Github pages suggest
something calle Jekyll as a generator for the site &amp;ndash; much better than manually coding
everything up in HTML and linking stuff by hand.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s a nightmare.&lt;/p&gt;

&lt;p&gt;Why, in 2017, is it harder to install some dinky piece of software than it was 20 years
ago? Why do I need to download half the packages on the internet to resolve dependencies?
And what do I do (as it invariably happens) when somthing falls over with a cryptic error
message? It&amp;rsquo;s enough to turn you into a grumpy old man.&lt;/p&gt;

&lt;p&gt;Then one of my twitter friends recommended Hugo. Installation was fine, only then github
was confused. It is already well known that &lt;code&gt;git&lt;/code&gt; is not exactly the pinnacle of usability,
and it doesn&amp;rsquo;t help that it seem to become the default tool for any command-line job. I can
see that a website can be modelled with a repository, and it&amp;rsquo;s probably a good idea, but
git seems such an overkill for 90% of what you would want to do.&lt;/p&gt;

&lt;p&gt;Several attempts and hours later I have a basic site running. But, no white space between the
date and title of the blog post. And no idea how to fix that. Gah. Maybe it would have been
better to just hand-code everything.&lt;/p&gt;

&lt;p&gt;It is understandable that website generation is complex. This can easily be seen by the number of
generators available for doing so. Everybody who tries to use one seems to find it complicated and
illogical, and then writes their own. Which slowly grows into a behemoth that is complicated and
illogical.&lt;/p&gt;

&lt;p&gt;It might sound arrogant and elitist, but I think software engineering is harder than many people
think. It&amp;rsquo;s always easier to start a new project than to maintain and curate an existing one. As
a logical consequence we have a proliferation of software projects, mostly doing the same things
in different ways. And few of them are of a high enough standard to be properly usable.&lt;/p&gt;

&lt;p&gt;I really wanted to end this post on a constructive note, but I don&amp;rsquo;t think I&amp;rsquo;m able to.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title> First Post</title>
      <link>https://ojmason.github.io/blog/first-post/</link>
      <pubDate>Mon, 27 Mar 2017 14:40:17 +0100</pubDate>
      
      <guid>https://ojmason.github.io/blog/first-post/</guid>
      <description>

&lt;h1 id=&#34;first-post&#34;&gt;First Post&lt;/h1&gt;

&lt;p&gt;I&amp;rsquo;m trying blogging again &amp;ndash; twitter is a bit too sparse for longer rants. And this seems
to be a time for rants. Lots happening in politics.&lt;/p&gt;

&lt;p&gt;Anyway, here we go again.&lt;/p&gt;

&lt;p&gt;Update: I&amp;rsquo;m also going through my previous WordPress log to gather some articles that
might still be interesting, so there will be older articles going back almost a decade
appearing here.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>about me</title>
      <link>https://ojmason.github.io/about/</link>
      <pubDate>Mon, 27 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/about/</guid>
      <description>&lt;p&gt;Hello, I&amp;rsquo;m Oliver. I am a computational linguist, interested in applications
of NLP and AI in understanding texts and conversing with human beings.&lt;/p&gt;

&lt;p&gt;I am a passionate dinghy sailor, sailing an Enterprise or a Laser at Midland
Sailing Club in the UK. I am also a qualified Dinghy Instructor, and run the
RYA training centre at the club.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Solving Combinatory Problems with Erlang</title>
      <link>https://ojmason.github.io/blog/solving-combinatory-problems-with-erlang/</link>
      <pubDate>Sun, 10 May 2015 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/solving-combinatory-problems-with-erlang/</guid>
      <description>&lt;p&gt;My daughter had some maths homework the other day:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;You have 4 bags, each full of the numbers 1, 3, 5, and 7 respectively.
Take 10 of the numbers that when added up make 37. What numbers are they?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So far so good: that sounds easy enough. But a bit of trial and
error quickly leads nowhere. Something can’t be right. So, let’s
get the computer to work it out.&lt;/p&gt;

&lt;p&gt;As I haven’t done much Erlang recently I thought I’d give it a go.
And, during a casual glance at Armstrong’s &lt;em&gt;Programming in Erlang&lt;/em&gt; I
thought I’d finally understood list comprehensions, so I wrote the
following program:&lt;/p&gt;

&lt;pre&gt;
-module(comb).
-export([result/0]).
result() -&gt;
[{A+B+C+D+E+F+G+H+I+J,A,B,C,D,E,F,G,H,I,J}||
A &lt;- [1,3,5,7],
B &lt;- [1,3,5,7],
C &lt;- [1,3,5,7],
D &lt;- [1,3,5,7],
E &lt;- [1,3,5,7],
F &lt;- [1,3,5,7],
G &lt;- [1,3,5,7],
H &lt;- [1,3,5,7],
I &lt;- [1,3,5,7],
J &lt;- [1,3,5,7],
A+B+C+D+E+F+G+H+I+J =:= 37].
&lt;/pre&gt;

&lt;p&gt;I declare a module with one function, &lt;code&gt;result/0&lt;/code&gt;. This finds me ten
variables that can take any of the four specified values and add
up to 37. Simples!&lt;/p&gt;

&lt;p&gt;The list comprehension has ten generators, and one filter; it will
return a tuple with the sum and the individual variables’ values.&lt;/p&gt;

&lt;pre&gt;
Erlang R16B01 (erts-5.10.2) [64-bit] [smp:4:4] [async-threads:10] [hipe] [kernel-poll:false]
Eshell V5.10.2 (abort with ^G)
1&gt; comb:result().
[]
2&gt;
&lt;/pre&gt;

&lt;p&gt;WTF???! An empty list?! So I try changing the 37 to another value, like 36.&lt;/p&gt;

&lt;pre&gt;
3&gt; comb:result().
[{36,1,1,1,1,1,3,7,7,7,7},
{36,1,1,1,1,1,5,5,7,7,7},
{36,1,1,1,1,1,5,7,5,7,7},
{36,1,1,1,1,1,5,7,7,5,7},
{36,1,1,1,1,1,5,7,7,7,5},
[etc, etc].
&lt;/pre&gt;

&lt;p&gt;So it does work! Only, there doesn’t seem to be an answer to the
question. And with a bit of logical reasoning it is obvious: when
adding two odd numbers, you get an even number. So adding ten odd
numbers also yields an even number, but 37 is odd.&lt;/p&gt;

&lt;p&gt;What I learnt from this exercise: thinking about the problem
beforehand can save you time, as there was no need to write a program
at all. But then, I did get to use list comprehensions, and have
learnt how powerful they are. And it neatly shows Erlang’s Prolog
roots as well.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Party Politics and the Emperor&#39;s New Clothes</title>
      <link>https://ojmason.github.io/blog/party-politics-and-the-emperors-new-clothes/</link>
      <pubDate>Sat, 24 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/party-politics-and-the-emperors-new-clothes/</guid>
      <description>&lt;p&gt;On Twitter I came across a link to the (German) &lt;a href=&#34;http://www.cdu.de/sites/default/files/media/dokumente/regierungsprogramm-in-leichter-sprache-btw13.pdf&#34;&gt;CDU Wahlprogramm in “Leichter Sprache”&lt;/a&gt;.
This is the conservative party’s election manifesto in a German
version of basic English, to make it more easily comprehensible to
people who have difficulty understanding standard German.&lt;/p&gt;

&lt;p&gt;At first I thought it was a parody, as it sounded truly ridiculous
and patronising. As if you had to explain to your 4-year-old what
the world was like. Then I found out about the “&lt;a href=&#34;http://de.wikipedia.org/wiki/Leichte_Sprache&#34;&gt;Leichte Sprache&lt;/a&gt;”
(literally: “&lt;a href=&#34;http://en.wikipedia.org/wiki/Plain_language&#34;&gt;easy/plain language&lt;/a&gt;“), which I had not heard of before.
And in principle it is a noble thing to do, to make your manifesto
more accessible. I’m sure &lt;a href=&#34;https://www.mtholyoke.edu/acad/intrel/orwell46.htm&#34;&gt;George Orwell&lt;/a&gt; would approve!&lt;/p&gt;

&lt;p&gt;But the real reason why it does seem strange is rooted in the nature
of political language: normally, language is used to obfuscate and
manipulate, especially in complex areas such as politics. Fancy
words are used to disguise true intentions and get people to vote
for you. But take away the fancy words and complex phrases, and
suddenly all is out there in the open, plain to see for everybody.
And it turns out to be pretty meaningless. “Everybody should have
a good job. And earn enough money. That needs to be written down.”
But no mention about minimum wage or anything concrete to turn those
vacuous phrases into reality.&lt;/p&gt;

&lt;p&gt;Some other people on twitter reacted in the same way as I did; often
I guess out of the same ignorance of the “easy language” concept.
That’s a shame, but partly rooted in the patronising way the sentences
come across. But it also shows that the (undesired) outcome of this
publication is that people tend to not take the content seriously.
Because there is not much of it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Critical_discourse_analysis&#34;&gt;Critical Discourse Analysis&lt;/a&gt;
would lose a big part of its subject
area if politics would switch wholesale to “easy language”. No more
subconscious manipulation between the lines, no more obfuscation
about who does what to whom. So after some reflection I applaud the
CDU for making their manifesto available in this way, as it unmasks
them as the patronising right-wingers they are, with their overly
simplistic world view about pretty much any subject area in current
politics. No more hiding behind fancy words that foreigners are not
welcome. No more vague claims about surveillance cameras stopping
crime. It’s all there, in plain language. And other parties seem
to have done the same.&lt;/p&gt;

&lt;p&gt;Now the only thing that needs to go is the disclaimer at the
beginning: that this plain version is not the real manifesto, and
only the real one does count. I would prefer it to be the other way
round.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Every Test is Valuable</title>
      <link>https://ojmason.github.io/blog/every-test-is-valuable/</link>
      <pubDate>Tue, 17 Jul 2012 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/every-test-is-valuable/</guid>
      <description>&lt;p&gt;After reading Graham Lee’s &lt;a href=&#34;http://www.amazon.co.uk/gp/product/B007RNK0W6/ref=as_li_ss_tl?ie=UTF8&amp;amp;camp=1634&amp;amp;creative=19450&amp;amp;creativeASIN=B007RNK0W6&amp;amp;linkCode=as2&amp;amp;tag=phrasysnlp-21&#34;&gt;Test-Driven iOS Development&lt;/a&gt;; (disclaimer:
affiliate link) I have (again) adopted a test-driven approach to
software developing. Whenever I create a new class I write a bunch
of tests exercising the class’ properties. One might question the
value of this, because there is not really any reason why those
should not work. However, having such a test in place just uncovered
an as-yet unnoticed bug I introduced in a project.&lt;/p&gt;

&lt;p&gt;Originally the class property in question was going to be set in
the &lt;code&gt;init&lt;/code&gt; method, so I tested for the presence of the property
after creating an instance of the relevant class. Easy pass. Weeks
later I did something (and forgot to run the test suite). Today I
did something else, and this time I did run them. Hey presto, failed
test. And completely unexpected, because it was the property
exercising one. How on Earth did that happen?&lt;/p&gt;

&lt;p&gt;Upon closer inspection I tracked it down to a refactoring, where I
extracted some code from &lt;code&gt;init&lt;/code&gt; as there were now multiple ways an
object could be initialised. The failing code was part of that, and
I realised that it was called from the new &lt;code&gt;initFromFile:&lt;/code&gt; method,
but &lt;em&gt;no longer from the default initialiser&lt;/em&gt;. Easy mistake to make.
And had I run my test-suite more consistently, my application would
not have been with a potential bug in the mean-time.&lt;/p&gt;

&lt;p&gt;What did I take home from this?&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Even Mickey-Mouse-tests are valuable. No test is too small to be useful
(provided it’s actually testing something valid).&lt;/li&gt;
&lt;li&gt;The test-suite should really be run after every change. I’ll have to check
if I can get Xcode to run them automatically after each compilation…&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The longest Chip in the World</title>
      <link>https://ojmason.github.io/blog/the-longest-chip-in-the-world/</link>
      <pubDate>Sat, 18 Dec 2010 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/the-longest-chip-in-the-world/</guid>
      <description>&lt;p&gt;In his &lt;a href=&#34;http://www.amazon.co.uk/gp/product/0465045669?ie=UTF8&amp;amp;tag=phrasysnlp-21&amp;amp;linkCode=as2&amp;amp;camp=1634&amp;amp;creative=19450&amp;amp;creativeASIN=0465045669&#34;&gt;Metamagical Themas: Questing for the Essence of Mind and
Pattern&lt;/a&gt;,
Douglas Hofstadter talks about &lt;em&gt;numerical literacy&lt;/em&gt;, the
ability to understand large numbers. This is especially important
when state budgets are thrown around which deal with billions of
pounds or euros. At some point you just lose all feeling for
quantities, as they are all so unimaginably large and abstract. He
then goes on to pose some rough calculation questions, such as “How
many cigarettes are smoked in the US every day?” – you start with
some estimates, and then work out an answer, which might be near
the order of magnitude of the right answer (which we of course don’t
know). Quite an interesting and useful mental exercise.&lt;/p&gt;

&lt;p&gt;Yesterday we had Fish &amp;amp; Chips for dinner. The girls were comparing
the sizes of their chips, and then we came on to the topic of the
longest chip in the world. Obviously, with ‘proper’ chips this is
limited by the &lt;a href=&#34;http://www.france-today.com/2009/12/longest-chip-in-world-isof-course.html&#34;&gt;size of the source potato&lt;/a&gt;.
However, assuming that industrially
produced chips are made of mashed potato formed into chip-shapes,
there is not really any fixed limit on the length. So, thinking
of Hofstadter, I asked them how many potatoes we would need to make
a chip that spans around the whole world.&lt;/p&gt;

&lt;p&gt;Rough assumptions: one potato contains enough matter to produce
10cm worth of chip (the thickness is not specified). So, how many
potatoes do we need for one metre of chip? This is also useful to
practice basic primary-school-level maths… – 10. How many for a
kilometre? 10,000. How many kilometres do we need to span the world?
Roughly 40,000 km. So how many potatoes do we need? 400 million.&lt;/p&gt;

&lt;p&gt;The next question is whether there are enough potatoes in the world
to do this. Assuming a potato weighs 100g, how much do our 400
million potatoes weigh? 40 million kilogrammes, or 40,000 (metric)
tons. What is the world’s potato production? According to &lt;a href=&#34;http://en.wikipedia.org/wiki/Potato&#34;&gt;Wikipedia&lt;/a&gt;,
this is 315 million metric tons, so plenty enough. Now, if we were
to turn the annual potato crop into one long chip, how many times
would it go round the Earth? 7,875 times.&lt;/p&gt;

&lt;p&gt;So, with a bit of basic maths (and Wikipedia for the data) you can
make maths exciting for kids, practice how to multiply and divide,
teach problem-solving, and have fun at the same time. And they also
get a feeling for numbers: 400 million – that’s how many potatoes
you need for a chip to span the Earth.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Sentence Disambiguation -- Modality to the Rescue!</title>
      <link>https://ojmason.github.io/blog/sentence-disambiguation----modality-to-the-rescue/</link>
      <pubDate>Thu, 12 Nov 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/sentence-disambiguation----modality-to-the-rescue/</guid>
      <description>&lt;p&gt;I’m currently reading a new book on iPhone development, &lt;a href=&#34;http://www.amazon.co.uk/gp/product/1430224037?ie=UTF8&amp;amp;tag=phrasysnlp-21&amp;amp;linkCode=as2&amp;amp;camp=1634&amp;amp;creative=6738&amp;amp;creativeASIN=1430224037&#34;&gt;&lt;em&gt;iPhone
Advanced Projects&lt;/em&gt;&lt;/a&gt;
 by Apress. I will probably talk about that book
in a later post, but today I will just focus on one sentence I came
across on page 212:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;I also adore the capability that I have to flag articles from folks I follow on Twitter and save them to Instapaper.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This sentence has (at least) two readings, which are probably only
obvious to a linguist (and who else would care?); I highlight the
differences by adding commas:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;I adore the capability, that I have to flag articles…&lt;/li&gt;
&lt;li&gt;I adore the capability that I have, to flag articles…&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the first case you adore the capability. And the capability is
that you have to do something (flag articles). Sounds rather odd,
doesn’t it? The second case is more clear-cut and easy to understand:
you can flag articles, and that’s the capability you have and adore.&lt;/p&gt;

&lt;p&gt;So in terms of &lt;a href=&#34;https://en.wikipedia.org/wiki/Pattern_grammar&#34;&gt;pattern grammar&lt;/a&gt;,
you’re either looking at &lt;em&gt;N that&lt;/em&gt; or
&lt;em&gt;N &lt;em&gt;to&lt;/em&gt;-inf&lt;/em&gt; with &lt;em&gt;capability&lt;/em&gt;. If you consult the &lt;a href=&#34;http://www.amazon.co.uk/gp/product/1424008255?ie=UTF8&amp;amp;tag=phrasysnlp-21&amp;amp;linkCode=as2&amp;amp;camp=1634&amp;amp;creative=6738&amp;amp;creativeASIN=1424008255&#34;&gt;Cobuild Dictionary&lt;/a&gt;,
you’ll find that &lt;em&gt;capability&lt;/em&gt; only occurs with the second pattern,
the &lt;em&gt;to&lt;/em&gt;-infinitive, so that you can rule out the first reading.&lt;/p&gt;

&lt;p&gt;Another possibility would be to look at it in terms of &lt;a href=&#34;http://en.wikipedia.org/wiki/English_modal_auxiliary_verb&#34;&gt;modality&lt;/a&gt;:
here we could argue that &lt;em&gt;capability&lt;/em&gt; prospects a modality of ability,
but &lt;em&gt;have to&lt;/em&gt; expresses obligation; the two don’t go together. Hence
the first reading sounds odd, as a capability does not usually force
you to do anything, but rather enables you. It could, however, be
used to signal sarcasm or irony, as in (the obviously made up) &lt;em&gt;I
really like that my new computer gives me the capability to have
to save my work every five minutes&lt;/em&gt;. This is clearly an odd sentence,
suggesting that modality works along similar lines as &lt;a href=&#34;http://en.wikipedia.org/wiki/Discourse_prosody&#34;&gt;discourse
prosody&lt;/a&gt; as described by Louw (1993) [for the full reference follow
the previous link].&lt;/p&gt;

&lt;p&gt;Here we have discussed two ways of disambiguating a sentence, one
based on grammatical properties (or typical environments), and one
on a non-syntactic phenomenon (modality). Pattern grammar allows
us to identify what the typical usage would be, whereas modality
explains to us why the first reading is at odds with the corresponding
words. Now all we need is a ‘pattern grammar’ for modality!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Collocations -- Do we need them?</title>
      <link>https://ojmason.github.io/blog/collocations----do-we-need-them/</link>
      <pubDate>Tue, 03 Mar 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/collocations----do-we-need-them/</guid>
      <description>

&lt;p&gt;The concept of collocation was introduced in the middle of the last
century by J.R. Firth with his famous quote “You shall know a word
by the company it keeps”. Words are not distributed randomly in a
text, but instead they stick with each other, their ‘company’.
Starting in the late 1980s, the increased interest in collocation
by computational linguists and others working in NLP has lead to a
proliferation of methods and algorithms to extract collocations
from text corpora.&lt;/p&gt;

&lt;p&gt;Typically one starts with the environment of the target (or node)
word, and collects all the words that are within a certain distance
(or span) of the node. Then their frequency in a reference corpus
is compared with their frequency in the environment of the node,
and from the ratio of frequencies we determine whether they’re near
the node by chance or because they’re part of the node’s company.
A bewildering variety of so-called significance functions exists,
the oldest probably being the &lt;em&gt;z&lt;/em&gt;-score, used by Berry-Rogghe in 1973;
later, Church and Hanks (1991) popularised mutual information and
&lt;em&gt;t&lt;/em&gt;-score, which now seem to have been displaced by log-likelihood
as the predominant measure of word association.&lt;/p&gt;

&lt;p&gt;The problem is: all these metrics yield different results, and
nobody knows (or can tell) which are ‘right’. Mutual information,
for example, favours rare words, while the &lt;em&gt;t&lt;/em&gt;-score promotes words
which are relatively frequent already. But apart from rules-of-thumb,
there exists no linguistic justification why one metric is preferable
to another. It is all rather ad-hoc.&lt;/p&gt;

&lt;p&gt;Part of this is that collocation as a concept is rather underspecified.
What does it mean for a word to be ‘significantly more common’ near
the node word as opposed to be there just by chance? In a sense,
collocations are just diagnostics: we know there are words that are
to be expected next to &lt;em&gt;bacon&lt;/em&gt;, and we look for collocates and find
&lt;em&gt;rasher&lt;/em&gt;. Fantastic! Just what we expected. But then we look at &lt;em&gt;fire&lt;/em&gt;,
and find &lt;em&gt;leafcutter&lt;/em&gt; as a very significant collocate. How can that
happen? What is the connection between &lt;em&gt;fire&lt;/em&gt; and &lt;em&gt;leafcutter&lt;/em&gt;? The
answer is: &lt;em&gt;ants&lt;/em&gt;. There are &lt;em&gt;fire ants&lt;/em&gt;, and there are &lt;em&gt;leafcutter ants&lt;/em&gt;,
and they are sometimes mentioned in the same sentence.&lt;/p&gt;

&lt;p&gt;This leads us to an issue which I believe gets us on the right track
in the end: the fallacy of using the word as the primary unit of
analysis. In the latter example, we’re not dealing with &lt;em&gt;fire&lt;/em&gt; and
&lt;em&gt;leafcutter&lt;/em&gt;, we’re instead concerned with &lt;em&gt;fire ants&lt;/em&gt;. Once we realise
that, then it is perfectly natural to see &lt;em&gt;leafcutter ants&lt;/em&gt; as a
collocate, whereas we would be surprised to find &lt;em&gt;engine&lt;/em&gt;, which
instead is a collocate of the lexical item &lt;em&gt;fire&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;So, phraseology is the clue. If we get away from single words, and
instead consider multi-word units, then we also have an explanation
for collocations. Single words form part of larger MWUs, together
with other single words. So &lt;em&gt;leafcutter&lt;/em&gt; often forms a unit with &lt;em&gt;ants&lt;/em&gt;,
as does &lt;em&gt;fire&lt;/em&gt;. More generally, MWUs such as &lt;em&gt;parameters of the model&lt;/em&gt;
are formed of several single words, and here we can observe that
&lt;em&gt;parameters&lt;/em&gt; and &lt;em&gt;model&lt;/em&gt; occur together. But they form a single unit
of analysis, and only if we break up this unit by considering single
words, then we can observe that &lt;em&gt;parameters&lt;/em&gt; and &lt;em&gt;model&lt;/em&gt; commonly occur
together.&lt;/p&gt;

&lt;p&gt;From this we can define a very simple procedure to compute collocations:
from a corpus, gather all the MWUs that are associated with a
particular word. Get a frequency list of all the single word items
in those MWUs, sort by frequency, and there we are.&lt;/p&gt;

&lt;p&gt;To conclude, &lt;em&gt;collocation is an epiphenomenon of phraseology&lt;/em&gt;, a
side-effect of words forming larger units. Phraseological units
contain multiple single words, and those are picked up by collocation
software, because those are the ones that commonly occur in a text
together. And the reason for occurring together is that they form
a single unit. Once we look at text in terms of MWUs, the need for
collocation disappears. Collocation just picks out the constituent
elements of multi-word units.&lt;/p&gt;

&lt;p&gt;One could of course argue that this is a circular argument, that
we are simply replacing a procedure to calculate collocations by
one that calculates MWUs. But the difference between those two
procedures is that MWU-recognition does not require complicated
statistics (which I find hard to see justification for), but instead
simply looks at recurrent patternings in language. MWUs are re-usable
chunks of texts, which can be justified on the grounds of usage.
Collocation is a much harder concept to explain and integrate into
views of language. And, as it turns out, we don’t really need it
at all.&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;dl&gt;
&lt;dt&gt;Berry-Rogghe, G.L.M. (1973)&lt;/dt&gt;
&lt;dd&gt;“The Computation of Collocations and Their Relevance in Lexical Studies.” in The Computer and Literary Studies. Eds. A.J. Aitken, R.W. Bailey and N. Hamilton-Smith. Edinburgh: Edinburgh University Press, p 103-112.&lt;/dd&gt;
&lt;dt&gt;Church, K., and Hanks, P. (1991)&lt;/dt&gt;
&lt;dd&gt;“Word Association Norms, Mutual Information and Lexicography,” Computational Linguistics, Vol 16:1, p 22-29.&lt;/dd&gt;
&lt;dt&gt;Firth, J. R. (1957)&lt;/dt&gt;
&lt;dd&gt;“A Synopsis of Linguistic Theory 1930-1955” in Studies in Linguistic Analysis, Oxford: Philological Society.&lt;/dd&gt;
&lt;/dl&gt;
</description>
    </item>
    
    <item>
      <title>Thinking Erlang, or Creating a Random Matrix without Loops</title>
      <link>https://ojmason.github.io/blog/thinking-erlang-or-creating-a-random-matrix-without-loops/</link>
      <pubDate>Thu, 26 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/thinking-erlang-or-creating-a-random-matrix-without-loops/</guid>
      <description>&lt;p&gt;For a project, my &lt;a href=&#34;https://ojmason.github.io/blog/fast-pfnets-in-erlang/&#34;&gt;Erlang implementation of a fast PFNET algorithm&lt;/a&gt;,
I needed to find a way to create a random matrix of integers (for
path weights), with the diagonal being filled with zeroes.  I was
wondering how best to do that, and started off with two loops, an
inner one for each row, and an outer one for the full set of rows.
Then the problem was how to tell the inner loop at what position
the ‘0’ should be inserted.  I was thinking about passing a row-ID,
when it suddenly clicked: &lt;code&gt;lists:seq/2&lt;/code&gt; was what I needed!  This
method, which I previously thought was pretty useless, creates a
list with a sequence of numbers (the range is specified in the two
parameters).  For example,&lt;/p&gt;

&lt;pre&gt;
1&gt; lists:seq(1,4).
[1,2,3,4]
2&gt; lists:seq(634,637).    
[634,635,636,637]
3&gt; lists:seq(1000,1003).
[1000,1001,1002,1003]
&lt;/pre&gt;

&lt;p&gt;Now I would simply generate a list with a number for each row, and
then send the inner loop off to do its thing, filling the slot given
by the sequence number with a zero, and others with a random value.&lt;/p&gt;

&lt;p&gt;But now it gets even better.  Using a separate (tail-)recursive
function for the inner loop didn’t quite seem right, so I thought
a bit more about it and came to the conclusion that this is simply
a mapping; mapping an integer to a list (a vector of numbers, one
of which (given by the integer) is a zero).  So instead of using a
function for filling the row, I call &lt;code&gt;lists:seq/2&lt;/code&gt; again and then map
the whole thing.  This is the final version I arrived at, and I’m
sure it can still be improved upon using list comprehensions:&lt;/p&gt;

&lt;pre&gt;
random_matrix(Size, MaxVal) -&gt;
  random:seed(),
  lists:map(
    fun(X) -&gt;
      lists:map(
          fun(Y) -&gt;
              case Y of 
                 X -&gt; 0; 
                 _ -&gt; random:uniform(MaxVal)
                 end
              end,
          lists:seq(1,Size))
      end,
    lists:seq(1,Size)).
&lt;/pre&gt;

&lt;p&gt;This solution seems to be far more idiomatic, and I am beginning
to think that I finally no longer think in an imperative way of
loops, but more in the Erlang-way of list operations.  Initially
this is hard to achieve, but with any luck it will become a lot
easier once one is used to it.  Elegance, here I come!&lt;/p&gt;

&lt;p&gt;Example run:&lt;/p&gt;

&lt;pre&gt;
4&gt; random_matrix(6,7).  
[[0,1,4,6,7,4],
 [3,0,5,7,5,4],
 [5,1,0,2,5,2],
 [4,2,4,0,3,1],
 [4,4,3,3,0,1],
 [5,7,3,2,2,0]]
&lt;/pre&gt;

&lt;p&gt;Note: I have used &lt;code&gt;random:seed/0&lt;/code&gt; above, as I am happy for the function
to return identical matrices on subsequent runs with the same
parameters. To get truly random results, that would have to be left
out. However, for my benchmarking purposes it saved me having to
save the matrix to a file and read it in, as I can easily generate
a new copy of the same matrix I used before.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fast PFNETs in Erlang</title>
      <link>https://ojmason.github.io/blog/fast-pfnets-in-erlang/</link>
      <pubDate>Sat, 14 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/fast-pfnets-in-erlang/</guid>
      <description>

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;http://en.wikipedia.org/wiki/Pathfinder_Networks&#34;&gt;Pathfinder Networks&lt;/a&gt;
(PFNETs) are networks derived from a graph
representing proximity data.  Basically, each node is connected to
(almost) every other node by a weighted link, and that makes it
hard to see what’s going on.  The Pathfinder algorithm prunes the
graph by removing links which are weighted higher than another path
between the same nodes.&lt;/p&gt;

&lt;p&gt;For example: A links to B with weight 5.  A also links to C with
weight 2, and C links to B with weight 2.  Adding up the weights,
A to B direct is 5, A to C to B is 4.  Result: we remove the link
from A to B, as the route via C is shorter.  There are different
ways to calculate the path lengths (using the &lt;a href=&#34;http://en.wikipedia.org/wiki/Minkowski_metric&#34;&gt;Minkowski &lt;em&gt;r&lt;/em&gt;-metric&lt;/a&gt;),
but you get the general idea.  The resulting PFNET has fewer links
and is easier to analyse.&lt;/p&gt;

&lt;p&gt;In Schvaneveldt (1990) an algorithm for computing PFNETs is given,
but it is rather complex and computationally intensive.  There is
an improved algorithm called Binary Pathfinder, but that is apparently
more memory intensive.  Not very promising so far, but then along
comes &lt;em&gt;A new variant of the Pathfinder algorithm to generate large
visual science maps in cubic time&lt;/em&gt;, by Quirin, Cordón, Santamaría,
Vargas-Quesada, and Moya-Anegón.  This algorithm is a lot faster
(by about 450 times), but has one disadvantage: speed is traded in
for flexibility.  The original Pathfinder algorithm has two parameters,
r (the value of the Minkowski metric to be used) and q (the maximum
length of paths to be considered).  The fast algorithm only has r,
and always uses the maximum value for q (which is n-1).  I know too
little about the application of PFNETs to say whether this is
important at all; for the uses I can envisage it does not seem to
matter.&lt;/p&gt;

&lt;p&gt;As added bonus, the algorithm in pseudo code in Quirin et al. is
very short and straightforward.  They’re using a different &lt;a href=&#34;http://en.wikipedia.org/wiki/Floyd-Warshall_algorithm&#34;&gt;shortest-path
algorithm&lt;/a&gt;
to identify, erm, shorter paths.  And then it’s very
simple to prune the original network.&lt;/p&gt;

&lt;p&gt;A picture tells more than 1K words, so here instead of 2000 words
the before and after, graph layout courtesy of the graphviz program:&lt;/p&gt;

&lt;p&gt;A network example (from Schvaneveldt 1990)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ojmason.github.io/img/pfnets-1.png&#34; caption=&#34;Fig 1: A network example (from Schvaneveldt 1990)&#34;/&gt;&lt;/p&gt;

&lt;p&gt;Here, each node is linked to every other node.  Running the PFNET
algorith on it, we get the output shown in the second figure.&lt;/p&gt;

&lt;p&gt;A PFNET generated from the previous graph&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://ojmason.github.io/img/pfnets-2.png&#34; caption=&#34;Fig 2: A PFNET generated from the previous graph&#34;/&gt;&lt;/p&gt;

&lt;p&gt;If you compare the output with the actual result from Schvaneveldt’s
book (p.&lt;sup&gt;6&lt;/sup&gt;&amp;frasl;&lt;sub&gt;7&lt;/sub&gt;), you’ll realise that it is not identical, and the
reason for that is that the example there limits the path-length,
using the parameters (&lt;em&gt;r&lt;/em&gt; = 1, &lt;em&gt;q&lt;/em&gt; = 2) rather than (&lt;em&gt;r&lt;/em&gt; = 1, &lt;em&gt;q&lt;/em&gt; = n-1)
as in the example shown here.  As a consequence, the link from N1
to N4 (with a weight of 5) disappears, because of the shorter path
(N1-N2-N3-N4, weight 4).  But that path is too long if &lt;em&gt;q&lt;/em&gt; is just
2, and so it is kept in Schvaneveldt’s example.&lt;/p&gt;

&lt;h2 id=&#34;implementation&#34;&gt;Implementation&lt;/h2&gt;

&lt;p&gt;It is not possible to implement the pseudo-code given in Quirin &lt;em&gt;et
al&lt;/em&gt; directly, as they use destructive updates of the link matrix,
which we obviously cannot do in Erlang.  But the first, naïve
implementation, is still quite short.  The input graph is represented
as a matrix (&lt;em&gt;n&lt;/em&gt; x &lt;em&gt;n&lt;/em&gt;) where each value stands for the link weight,
with zero being used to indicate non-existing links.  I have written
a function that creates a dot file from a matrix, which is then fed
into graphviz for generating images as the ones shown above.&lt;/p&gt;

&lt;p&gt;There are basically two steps: creating a matrix of shortest paths
from the input matrix, and then generating the PFNET by comparing
the two matrices; if a certain cell has the same value in both
matrices, then it is a shortest path and is kept, otherwise there
is a shorter path and it’s pruned. Here is the main function:&lt;/p&gt;

&lt;pre&gt;
find_path(Matrix) -&gt;
    Shortest = loop1(1,length(Matrix),Matrix),
    generate_pfnet(Matrix, Shortest, []).
&lt;/pre&gt;

&lt;p&gt;Next we have the three loops (hence ‘cubic time’!) of the Floyd-Warshall
shortest path algorithm to create the shortest path matrix:&lt;/p&gt;

&lt;pre&gt;
loop1(K,N,Matrix) when K &gt; N -&gt; 
    Matrix;
loop1(K,N,Matrix) -&gt;
    NewMatrix = loop2(K,1,Matrix,
        Matrix,[]),
    loop1(K+1,N,NewMatrix).

loop2(_,_,_,[],Acc) -&gt;
    lists:reverse(Acc);
loop2(K,I,D,[Row|Rest],Acc) -&gt;
    NewRow = loop3(K,I,1,D, Row, []),
    loop2(K,I+1,D,Rest,[NewRow|Acc]).

loop3(_,_,_,_, [], Acc) -&gt;
    lists:reverse(Acc);
loop3(K,I,J,D, [X|Rest], Acc) -&gt;
    loop3(K,I,J+1,D, Rest,
    [min(X, get(I,K,D) + get(K,J,D))|Acc]).
&lt;/pre&gt;

&lt;p&gt;The final line implements the Minkowski metric with &lt;em&gt;r&lt;/em&gt; = 1; this
could be expanded to include other values as well, eg &lt;em&gt;r&lt;/em&gt; = 2 for
Euclidean, or &lt;em&gt;r&lt;/em&gt; = ∞ (which seem to be the most common values in
use; the latter means using the maximum weight of any path component
along the full path).&lt;/p&gt;

&lt;p&gt;And here are two utility methods, one to find the smaller of two
values, and one to retrieve an element from a matrix (which is a
list of rows).  There is something of a hack to deal with the fact
that zero does not mean a very small weight, but refers to a
non-existing link:&lt;/p&gt;

&lt;pre&gt;
min(X, Y) when X &lt; Y -&gt; X;
min(_X, Y) -&gt; Y.

get(Row, Col, Matrix) -&gt;
    case lists:nth(Col,
      lists:nth(Row, Matrix)) of
        0 -&gt; 999999999;
        X -&gt; X
    end.
&lt;/pre&gt;

&lt;p&gt;And finally, generating the PFNET by comparing the two matrices
(again, awful hack included):&lt;/p&gt;

&lt;pre&gt;
generate_pfnet([],[],Result) -&gt;
    lists:reverse(Result);
generate_pfnet([R1|Rest1], [R2|Rest2], Acc) -&gt;
    Row = generate_pfrow(R1,R2,[]),
    generate_pfnet(Rest1, Rest2, [Row|Acc]).

generate_pfrow([],[],Result) -&gt;
    lists:reverse(Result);
generate_pfrow([C|Rest1], [C|Rest2], Acc) -&gt;
    case C of
        999999999 -&gt; C1 = 0;
        _ -&gt; C1 = C
    end,
    generate_pfrow(Rest1, Rest2, [C1|Acc]);
generate_pfrow([C1|Rest1], [C2|Rest2], Acc) -&gt;
    generate_pfrow(Rest1, Rest2, [0|Acc]).
&lt;/pre&gt;

&lt;h2 id=&#34;discussion&#34;&gt;Discussion&lt;/h2&gt;

&lt;p&gt;So this is the basic code.  It works, but there is scope for improvement.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;it only currently generates PFNETs with (&lt;em&gt;r&lt;/em&gt; = 1, &lt;em&gt;q&lt;/em&gt; = &lt;em&gt;n&lt;/em&gt;-1)&lt;/li&gt;
&lt;li&gt;there is no parallelism, hence it’s not really making use of Erlang’s strengths.&lt;/li&gt;
&lt;li&gt;the three loops don’t look very elegant, and could probably be replaced by list comprehensions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Because of the matrix being updated, it doesn’t look that easy to
parallelise the processing, but it would work at the level of
updating the individual rows.  If that can be done in parallel, it
would probably provide some speed-up (provided the matrix is not
of a trivial size).  So the first step would be to change the matrix
processing using &lt;code&gt;lists:map/2&lt;/code&gt;, and replacing this then by &lt;code&gt;pmap&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Once the code is up to scratch with full parallelism, and tested
on larger matrices I will probably put it up on Google Code in case
other people are interested in using it.  If you have any suggestions,
tell me in the comments!&lt;/p&gt;

&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;

&lt;dl&gt;
&lt;dt&gt;Schvaneveldt, R. W. (Ed.) (1990)&lt;/dt&gt;
&lt;dd&gt;Pathfinder Associative Networks: Studies in Knowledge Organization. Norwood, NJ: Ablex.
The book is out of print. A copy can be downloaded from the &lt;a href=&#34;http://en.wikipedia.org/wiki/Pathfinder_Networks&#34;&gt;Wikipedia page&lt;/a&gt;&lt;/dd&gt;
&lt;dt&gt;Quirin, A; Cordón, O; Santamaría, J; Vargas-Quesada, B; Moya-Anegón, F (2008)&lt;/dt&gt;
&lt;dd&gt;“A new variant of the Pathfinder algorithm to generate large visual science maps in cubic time”, Information Processing and Management, 44, p.1611-1623.&lt;/dd&gt;
&lt;/dl&gt;
</description>
    </item>
    
    <item>
      <title>Kids and Computers</title>
      <link>https://ojmason.github.io/blog/kids-and-computers/</link>
      <pubDate>Tue, 08 Jul 2008 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/kids-and-computers/</guid>
      <description>&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; This did not turn out as I hoped. The main reason being that an old ZX Spectrum
pre-dates the common availability of the Internet, and as soon as it was switched on the
question arose, how one could watch YouTube videos on this machine. So, abject failure.
Times have moved on, kids these days don&amp;rsquo;t know they&amp;rsquo;re born, good old days, it was so much
better when I was young, spoilt kids nowadays etc etc etc.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;Like many(?) computer people of my generation I started off with
the Sinclair ZX81, and then continued with the ZX Spectrum and
onwards to an Amstrad CPC, learning BASIC and Z80 machine code
(because those machines were actually pretty slow).  This also means
that I treat computers as tools, not passively using programs other
people have written, but writing my own stuff.  In time I moved on
with languages, from Pascal to C, a bit of Prolog, C++, Java, and
now Erlang.  But those endless hours typing in program listings in
BASIC have surely benefitted me.  Learning by doing.  Type in small
programs, change the code, observe what happens.&lt;/p&gt;

&lt;p&gt;My oldest daughter is seven, still half my age when I started with
computers, but from next year onwards they have ICT classes at
school; these basically seem to be ‘how to change fonts in MS Word’.
So I thought I’d better show her that a computer program is something
you write, not just something you use.  The only problem was, how
to do it?  Despite all its faults, BASIC seems to me to be a good
language to start with.  True, it has few control structures or
abstract data types, but it reads almost like English, and is only
a starting point after all.  And it’s only a first step in any case.&lt;/p&gt;

&lt;p&gt;The next question is how to get her interested.  Using a PC seems
like overkill, and you need to boot it up, and shut it down, and
then start a development environment etc.  And I guess it’s not
that easy to use graphics or sound as it was on the old Spectrum.
So, I went to ebay and bought a ZX Spectrum +2 for about £7.  You
plug it in, it’s on.  You can start typing.  You’re finished, you
unplug it.  No files to get corrupted.  And it’s got a built-in
tape recoder for storage.&lt;/p&gt;

&lt;p&gt;And, she is really excited.  It’s her very own computer, and she
already knows several commands.  All I need now is a few books with
BASIC programs, so that she has some material to type in.  In the
meantime I go with her through the manual, explaining the new
keywords, and she can already understand how a program is executed.
With any luck she’ll get hooked, especially once she learns how to
do graphics with it.  And the little games (‘guess the number’) are
also the right level for her, and maybe even her sisters.&lt;/p&gt;

&lt;p&gt;The only problem I can see is printing.  But I’ve got an emulator
on my laptop, which might be able to read in something she saves,
and I might be able to take it from there.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Programming and the Tower of Babel</title>
      <link>https://ojmason.github.io/blog/programming-and-the-tower-of-babel/</link>
      <pubDate>Sat, 31 May 2008 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/programming-and-the-tower-of-babel/</guid>
      <description>&lt;p&gt;&lt;em&gt;Update:&lt;/em&gt; I originally posted this article in May 2008, but it is still relevant. I have kind of
found a solution that I&amp;rsquo;m trying out now, namely moving to a fairly minimalist language, Scheme.
It is well suited to the main area I work in (NLP/AI), can be used purely functionally, and has
such a simple syntax that it should be relatively easy to write run-times for it on various platforms
if necessary.&lt;/p&gt;

&lt;p&gt;In the meantime, however, the need for using different platforms has become less pressing for me, as
I mainly do this as a work-related hobby nowadays.&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;I’m having a problem with languages. Not that there is something I
cannot do in my favourite language, but rather that there are
distinct ecosystems for various languages, and they are usually
fairly exclusive. For example, in order to program in a web context
you need either PHP, or alternatively be prepared to run your own
server if you want to use Java or Erlang. I’m not talking about
businesses or commercial operations here, just the private/academic/small
scale non-funded project range.&lt;/p&gt;

&lt;p&gt;Most of the language processing software I’ve written is in Java,
because it works well and can run almost everywhere (but not on
shared webhosts). It’s useful to distribute applications, as people
can use it on Linux, Macs and even Windows. But I’ve now pretty
much switched to Erlang, as I put high hopes on the future of
parallel programming, and I want my software to take advantage of
multi-core processors. However, Erlang programs are not as easily
shared and distributed as Java apps are. Problem.&lt;/p&gt;

&lt;p&gt;Ideally I’d like to write all my programs only once. This was kind
of the promise of Java, and it worked mostly. At some point I even
considered working on an interpreter for JVM bytecode written in
PHP (so that my Java classes would work without having to be re-coded
in PHP itself), but aside from the possibly terrible performance
it seemed too daunting a project. Maybe Erlang could be compiled
into JVM bytecodes? Of course you’d lose all the concurrency features
etc, but at least you could deploy it together with a Java app. A
bit like Scala, almost.&lt;/p&gt;

&lt;p&gt;If PHP wasn’t such a ghastly language I’d be happy to code everything
in that, but it seems too much of a sacrifice. But for most purposes
I would need at least those three:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Java for applications to be deployed to other people&lt;/li&gt;
&lt;li&gt;PHP for stuff running on shared webservers&lt;/li&gt;
&lt;li&gt;Erlang for research and cutting-edge stuff&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And there is little to no common ground between them. Sigh.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Replacing a stack with concurrency</title>
      <link>https://ojmason.github.io/blog/replacing-a-stack-with-concurrency/</link>
      <pubDate>Wed, 23 Apr 2008 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/replacing-a-stack-with-concurrency/</guid>
      <description>&lt;p&gt;For some language processing task I needed a reasonably powerful
parser (a program to identify the syntactic structure of a sentence).
So I dug out my copy of Winograd (1983) (&lt;em&gt;Language as a Cognitive
Process&lt;/em&gt;) and set about implementing an &lt;a href=&#34;https://en.wikipedia.org/wiki/Augmented_transition_network&#34;&gt;Augmented Transition Network&lt;/a&gt;
parser in Erlang.&lt;/p&gt;

&lt;p&gt;Now, the first thing you learn about natural language is that it
is full of ambiguities, and so there will always be several
alternatives available, several possible paths through the network
which defines the grammar. The traditional solution is to dump all
the alternatives on a stack, and look at them when the current path
has been finished with. You can either go depth-first, where you
complete the current path before you get the next one off the stack,
or breadth-first, where you advance all paths by one step at a time,
kind of pseudo-parallel.&lt;/p&gt;

&lt;p&gt;Having to deal with a stack is tedious, as you need to keep track
of the current configuration: which network are you at, what node,
what position in the sentence, etc. But then, it occurred to me,
there’s an easier way to do it (at least it’s easier in Erlang!):
every time you come to a point where you have multiple alternatives,
you spawn a new process and pursue all of them in parallel.&lt;/p&gt;

&lt;p&gt;The only overhead you need is a loop which keeps track of all the
processes currently running. This loop receives the results of
successful paths, and gets notified of unsuccessful ones (where the
process terminates without having found a valid structure). No need
for a stack, and hopefully very efficient processing on multi-core
machines as a free side-effect.&lt;/p&gt;

&lt;p&gt;I’m still amazed how easy it was to implement. I wouldn’t have
fancied doing that in Java or even C. For my test sentences I had
about 8 to 10 processes running in parallel most of the time, but
it depends on the size of the grammar and the length of the sentence
really. What I liked about this was that it seemed the natural way
to do in Erlang, where working with processes is just so easy.&lt;/p&gt;

&lt;p&gt;And also, another nail in the coffin for the claim that you can’t
use Erlang for handling texts easily!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Meteorites</title>
      <link>https://ojmason.github.io/blog/meteorites/</link>
      <pubDate>Wed, 09 Apr 2008 00:00:00 +0000</pubDate>
      
      <guid>https://ojmason.github.io/blog/meteorites/</guid>
      <description>&lt;p&gt;While clearing out a decade’s worth of paper from my office I came
across an article in the Feb 2003 issue of the university’s in-house
paper, &lt;em&gt;Buzz&lt;/em&gt;. In one article, the following extract struck me: &lt;em&gt;…artists
had acquired a meteorite from a meteorite dealer in Scotland.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Several questions come to mind: Can you make a living as a meteorite
dealer? I guess there are more meteorites hitting Earth as one would
think, not all of course big enough to make (no pun intended) an
impact. And why Scotland? Is Scotland especially prone to being hit
by meteorites?&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://curious.astro.cornell.edu/about-us/75-our-solar-system/comets-meteors-and-asteroids/meteorites/313-how-many-meteorites-hit-earth-each-year-intermediate&#34;&gt;Thanks to the web&lt;/a&gt;,
the first question can be supplemented by a
useful snippet of information: &lt;em&gt;Over the whole surface area of Earth,
that translates to 18,000 to 84,000 meteorites bigger than 10 grams
per year.&lt;/em&gt; But of course 70% of that end up in the sea, and most of
them never make it through the atmosphere in the first place. I
also don’t want to speculate how much demand there is for meteorites,
other than from conceptual artists and perhaps astrophysicists.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>